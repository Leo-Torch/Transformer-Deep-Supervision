{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28449,"status":"ok","timestamp":1666244425251,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"-6QrLCOTjzzW","outputId":"0fb57981-485d-4271-8664-b676aa10879e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s \n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 91.0 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 80.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 91.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 96.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 98.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 95.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 92.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 70.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 59.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 92.6 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 90.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 93.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=aed207a4b271eb87cbb8a8bd80c776bcf10758a4766140da73ddcdbfd373a54d\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 87.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 57.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.6 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n","\u001b[K     |████████████████████████████████| 441 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 88.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 95.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 90.8 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.9.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.6.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"]}],"source":["!pip install wandb\n","!pip install transformers\n","!pip install torch\n","!pip install sentencepiece\n","!pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1666244425251,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"gmus-ntxUXLM"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6100,"status":"ok","timestamp":1666244431348,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"A4OW0qKyjzzY"},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import pickle\n","import transformers\n","from datasets import load_dataset\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","import glob\n","import os\n","import wandb\n","import random, os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from transformers import AdamW\n","import torch\n","from transformers import get_cosine_schedule_with_warmup\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoModel, AutoTokenizer, AutoConfig\n","import re\n","from torch.nn import Module\n","from collections import Counter, defaultdict\n","from tqdm import tqdm\n","from torch import nn\n","import sys\n","import gc\n","from transformers import DataCollatorWithPadding\n","\n","\n","# From this Gist: https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n","def seed_everything(seed: int):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","   "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1631,"status":"ok","timestamp":1666244432976,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"lYX_09FZjzzc","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["11f1b0e39c524396b111e82f986b364a","7b646c9638d24367837d1e279f3b99f0","f0f9c190788144eca4920aae3acce3d2","a248852be4e0465ab4e717481e1a6127","050e4048b34645c7b9e0a2c7dc85dcf6","41292a553db64ee3b876cf9a9396ae8b","c199156a527141fa82d6370f8bf4ed32","073724166d3e4d50ad94c9532d77e462","ce93b716ab6242c39e34c87fa8a4d764","bebb3153c7f049ebaa9a01f8a845b326","f4efbcde03cf420487aa95a0b6243a65","398a705abf80414b878508c90f0d012e","9199eb0ac7144d59b0dbba3abc2f287a","bf3d90313b094654b85a725596d232f4","96426062d5904326b0778bef8dce50b8","2dd4804cb41c4548bd8e5ea67156181a","d4385b076b174108adc0700d1d5014f3","78f4165fe3e24a7eba5e408e10351edd","1b7b104cc4c44fdb92e5236a418711de","f080a254aaba48b78502e8d6ef234858","c53725da394a4c508a719602075629cf","5b00fc367f35490e8af763e13441e594","f5941a8d087546308792b5da4db052ff","e9d99dbecc89493aab658b98c0b1643a","5cb94fed351d434596320cb2f7aabfff","41512e8aaed24f808331af0e70693543","1e98828eb00745e0a53a32cb04d7bfe4","7edc7f7c98fc4f309fb422aff2d270c0","84ee15ea3e584fe486ec1017894b73d0","fe613a0d99c14387a5ce65fc5dda808c","1fa40ffd4b2b402cba7348f4169f8620","3c30e0993bce451e988d221ee133ac2a","6459b144543943888610be4d17a16cd9","6c4d8c7ee6e84397a41bc6be08ecbf68","36bfbbb1b2e24b83a7608230a485e3f8","923575645a5c404690e86ae2375d3e32","68a84fb8977a4335aae056b8c9fb5372","bc0a1748b6c64207bbbc4461a8cff7a5","ed6e73738e4b4639b4549fe28bf1a4cb","f5bdf1d15acd4334817e434d0e4fdc97","7270f970347a4ccc92ee949236295b0e","0b46a3a9c955498b8d178ba3a34a8c26","71ac9b98da224b1182d8cf2c351f6d8e","574b6714fab94763be99107455a63ab5"]},"outputId":"94915c0b-4b42-48e5-ce33-ff08a0766a45"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f1b0e39c524396b111e82f986b364a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398a705abf80414b878508c90f0d012e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5941a8d087546308792b5da4db052ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4d8c7ee6e84397a41bc6be08ecbf68"}},"metadata":{}}],"source":["CFG = {\n","    \"seed\": [42, 12, 48, 55, 83],\n","    \"model_name\": \"bert-base-uncased\",\n","    \"max_length\": 512,\n","    \"lr\": 2e-5, \n","    \"output_lr\": 5e-5,\n","    \"batch_size\": 32,\n","    \"epochs\": 20,\n","    \"num_warmup_steps\": 0.0,\n","    \"dataset\": \"rte\",\n","    \"type\": \"+ DS + CLS + AAM\",\n","\n","    \"patience\": 10,\n","    \"dropout\": 0.5,\n","    \"grad_accum\": 1,\n","    \"layer_start\": 1,\n","    \"pooler\": \"deep\",\n","    \"aux_weight\": 0.5,\n","\n","    \"weight_decay\": 0.3,\n","    \"grad_norm\": 1000,\n","    \"optimizer\": \"AdamW\",\n","    \"scheduler\": \"linear\",\n","}\n","CFG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG[\"model_name\"])"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1666244433106,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"iLRAH4l_dpVE"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233,"referenced_widgets":["fdb148737c9344ed86394b0ea629aecf","a2f7b7b54adf4da0aefdcc9af0ef00c7","0485e767f9a144598d94d98d70187a66","2aba8695fafc4455b23b70717d12e733","6c939388d5604e788f22e2f43d7d9be0","609a69ba23ea43efaca0c8f331d4ada1","5f39f6f021ac49e7b521fa5b79decd92","5200ac118f494ff1aab4ad9eb1a0d05c","cc53030cb7eb4464b023b83d1d505d81","f36125d55ef946dc971c1eec38948a3a","88d60daf5f254a0fa13b0f0b0e9dd1be","3149f0f38af4434e952e66092e6c00da","aec1cf7bb1de46b59c442af4a34c7840","872fa7b95469496abacf1f9cd91012db","2a6265c85e5e48f4a18b5e3babbc0f85","5247899cc8bc46c792ca18ae392b367d","3b840c7cfab042c58c4bc7dea2f9bd07","e01fac78d21e4432b3f33f0ab596360e","1eca2b205fdf49a98d893182dca0b2cd","5006200a961a45e9875eb5a12fc64a25","725570f3eade41c9ba62cfd76fe7411e","984658afb74a42af8a339490e1999acd","e50e8a2cb59b4a1aae94e9ca685ed2d8","ccca90574ac14e758d1a2c6c3121ab33","1bca0faf847c49f6a5cfd9049f862620","97a2c5e36c594774805b1028dca5325f","9fd6223accd640bf956f862c30660657","d5f121bfcf5f40b8beb5d12591912e37","68b9e533a5a4488c93ea1162f797de5a","3d6b130c30a249e99af610d5cfe5a48d","84af968f4f874117b91426cac42814fc","3edc6b9cfc874feab738709cedd9ec7c","82e64da19b17441db6ce7423a92d0342","420f934c525d48ef9847311f76de971e","c2b8b93ec88d4b4fadfb0bbd47e040d2","98e0eeaa1f4e45bc8e05aea33f0f3756","17ed059ba1ba431e8e53bfa9308021bb","08b81912ed3e444bb086746071902a24","281ee8a4628c4c20b677c0f50f040779","71a4922ab9434b21a3a99f774e33d4f0","919752d73cd546e8b9a66639b95028e5","210c50fc752d47df815dca424e861fee","c386996b548841c7bbe6f0d8b356b71c","0dfe7dcc9c2a4988b836584ebe319a0d","682afebb7e204bd1b5cc7010fc1c9304","64ff9775c3c44df99f495bdc12877748","cc6c58e6beca46618bd5e3a799ea13bf","1cf09787aaf54753b0ed996a482c0924","6ce786edf6a8498a8ffe9c92a8657cac","d7e07c44cfa74da69391e0a4cadfda79","3b4adf1bcb3a45af9d85920ddc3241f2","be50f266fa8c40f7af6e3d58f4de7330","26fdf1f28cbb4c9aa434320044373836","78a1bca8c95c4d5dac8882c8580cf5fd","a77cb6b371ae401381bd3cc7604e096e","8948084a9abe4673ba3b95d28f2a6b7c","8e42bb9c2aac4ec4950677ffd7661799","f3690b2815174b908f1a9be1a7dbfe26","a5eed7781c774e44afa5500209728ad3","0c19ff9fa08f44a3846a6cd7a1feb3a3","dd6aa21a64174b92bed3747de2c63a85","994e2b4d1a9b436aab6162015d0af8c4","1bbe9000d9454406a5bb840675d5e31e","8893a1c01cd54910b9c9f826cf2eeba8","6f670dacee284026902ae99508b65a83","1e7b28a3c3e841c9b4437cc478643780","8eb2887355f24b70bf26d39ef7628a16","cfe42ba11dce4772ab855897b3bb76b2","8c569b2073fa4562aa506f2be184a1f1","5c7b6624964e4a4eaa480eee80ca5ddb","a0ee4aa123e345f49096b4d9e1e80843","05a6f4b180434f6189256f5993f4477d","f90dda0ac00741c0a533b60859056beb","ee405f3411424215a7b6330d6d582b63","4913b8dbf3334bef8a1fd35966154038","376b10741dd849259cc3739b6b5f6438","011d1a16df6a4272aafbf58667d6c014","ed8f312f56e140d7ba1b02f5828cb4c9","7b6292a8b6fb46d8a013a20464649b5e","d2adb16e0b7f4d3c92d146dbd6f864f8","2bad6d65585845e38b76ef4ce4a914cd","c97fe2b52b374519a3bebe57fb64f9f2","7260598dfb284378a05e7a78728fe0ed","a000eea6a0e843fe96df1c73a47eee6b","d0f78b01c04d42ac8652edd6ca9c9d95","fc95751c114346b5a4c1219bf10372a4","ec496839eb884891b0c3cb9f0d966ce3","f10e2101b1444431bd56d490a4b5ba9c"]},"executionInfo":{"elapsed":2231,"status":"ok","timestamp":1666244435335,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"IW2NU8WKaprn","outputId":"98f4468d-98e4-468d-ae08-dcc3271cf9e0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb148737c9344ed86394b0ea629aecf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3149f0f38af4434e952e66092e6c00da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/22.0k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e50e8a2cb59b4a1aae94e9ca685ed2d8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset glue/wnli (download: 28.32 KiB, generated: 154.03 KiB, post-processed: Unknown size, total: 182.35 KiB) to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/29.0k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"420f934c525d48ef9847311f76de971e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/635 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"682afebb7e204bd1b5cc7010fc1c9304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/71 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8948084a9abe4673ba3b95d28f2a6b7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/146 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eb2887355f24b70bf26d39ef7628a16"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed8f312f56e140d7ba1b02f5828cb4c9"}},"metadata":{}}],"source":["data = load_dataset(\"glue\", CFG[\"dataset\"])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1666244435335,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"rk66iMNU9bLY","outputId":"ab034e14-760f-4079-f421-153168c1c145"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 635\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 71\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 146\n","    })\n","})"]},"metadata":{},"execution_count":7}],"source":["data"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1666244435562,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"gxUD8XrX3SuY"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(CFG[\"model_name\"])\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["59cc7d68291442e3883b264c57f235d4","c95104a01c1b42f78477ab50f50f7bf4","7692c19bde4c474ca734ca8ac52f946a","8d99985ad6c7452fa2e22f97fbe9a9e2","743a4ebd1a4949dea03e7004179eba8b","28ae51470bb74d2f9d48e84e68c6a80b","ebd2c603d2374d97b8da02b4f990322d","fd803936ba9b48fa89431014128b895b","2c6c2021d0a542b499af67607a907882","089c3034152c4496bcd69f63af329b93","d4d9522cf1f74499b8affb6549be062c","563bd3b228144cdf9f9cacfa1ef5bda3","74f27cbe76bf4fd8b88348255f92dbf1","4e6fb87691354ebabb207abe5bf79c70","97c774933fbc41e38676eb2d395f5527","a691648f1a254f298da9cc52217fb85e","a269031086d74039919a37d352108f6c","6db88c663ba94e78aecc60ebfe780148","52d6507851ea464da1e48eec8ae77b63","47390c882f664c108785980121286eb3","ec96832905154cfabe143c2780c0b2d1","4e7aab0f51954a02b48ae16adff509af","e2894677b67a4eeaa13d3aea2f1370d1","51e3c024514a48fb9d2ecd7bf28117af","921a7e3f17e648b79e242e9b24551918","9f57996c2f964a47b85acd4da9effc34","5a395d38e16c403eb9d94b3a25ab34d2","dec9e750291f4b7b94448c2fd2f49181","7be5ba863c5a4befa3f8878636462cb5","48762ef76ed241ddb1fb1aaadb3f701c","227821b436a64a418071bd11f60744c9","d408e23b7c0d4db4ba850317cd2a9d0e","f8fff800dd404b929036b676c31524b2","c57122d834594856b0ee3c384b925406","e859fb6c7e2a4c23bbfc82265baa3af2","beb822d9b96f4f16ad843289133b437b","37353a6131f5423c8777cd3c97844a96","9b563da356624bbbbe26ede8d754ff12","8bfdc0611e8f4b398650aefe8efe3e16","7dfcdd278d0d49dea86349b762f91b54","f54821a5e234489abadd7db4d1bc2c19","477a2422198743f8962379cc8607d639","f7a3f2ecf15543d4aeee27a23fecff11","707475ee171c4132966f11b0e3455f3f","e558ced6ebcc46c3937d0413f78304dd","d1a2c4113e034888b7c73ace0af4811a","1a73e00d24074d62bd2e49292d72404e","a2524333ece8448a86d0d213df1f05dc","1d682577364746dcb3e7730372f3da73","54ab1fa881bb47a085b339b00eabe1ad","95f88395ce194b519e3911e08fbb9d17","a122125680c74398b682cc76a4687773","989dd129bc8145ceaf1750cd9373b964","ee8046d32cc949aea74a6089f9fa0e4a","08b20c84d5a54f0b91b9072bc7e85f77","ecd47314ec2441d491641609e46996ff","bf7ba6428cce4160ad1587e81b86282f","5bef26a4519648f1b53f79217b175594","e365e31d7a0746139231ff328b1983c4","75878a03073a4ee4b5bce7b31a83a28a","c0340f423ed34da08c62c54a1d8fca0e","1f4b80fc0dee4046977d6dc034280202","45db3374b2a14ddb96c540c3bf5f62d6","ff7addbe02594389abc6a626d7c233f7","443e1e7538d44164863907f602e7759c","68798f7d8be140609c730655e5eb5686","53ce5c73b7f44ea3a99985bb45260905","41aba5ee54824a91b82a816ae9f6e155","6b8d1d481b2349d5891981a04bb438c4","e08e8789b74b4da1ab5b526f6ab684d2","1cad84b31db64218b533d0f2dec6d460","9c026381b9d04b17836f1c55c35cd619","b65df35d661d4447b3cdb19a8a684140","2729fb4ab9e94d03aaa11d5a7ae6b708","bddc94aa3ec54fbfbd9b2bccfd363321","2176c70770ea4a2b9c22c54193521727","ec26de6a90374a32b833fa6292cf78db","67cb95ffe32d48b6b594b4a652e42857","d40c51dcc7b04cba8a0131f19782eebc","adaa54ef20bc47cd8188f76fa1b322a5","fe2f299c951445edbe5221fb51cd57ba","4d7266ffa14e45e0959d8d11ebaac5b7","68d19d5cf0d64aa19f471615ddcc3ffd","cbcf3f63f23246b48fed256e58bfa393","547e46d05af7402ca06980169b40321a","a420116441f344f18a5ac8f3ad497ce3","9110e91e4e7c4036a8d9c88d04dcc81c","1664581f0ae24ed18d53041b4831188f","6fd11182a4bf4053a54e77965db7bc9d","344f90a8e6084a20800e4c561ec90066","a759a23c165249ee8782cb331fca6c5b","2696194cb66c463caa1bde8a213a5440","31d0cc8bbb4a45f898aa4bf9c47335f5","24e3444ea0dc4ac4876ce7f4ff011f27","5b5bce0ac27340b98021a297a7ba38cc","b76a18664b30408b8a15e87942a8e452","6e0f3e0920db4497a1879ec8c659432c","af075d6f8abc49b7ae336de7a0178796","1f1f266fef2740babaa1d82e0df6cf00","34449fb324244c89a3014c274845117d","4596ee37c68c4415a83d5b4143a96fe6","5bd071267fb9493db2a774254c16b20f","34f743ef53144fb797383c7377d03d88","5f6e1a969ee74d49913b7ee0dc3e4ba6","7d4e1919bf014f5aa4f197254f6388f0","8243e5c2f77a401a8e38e0dd1b272037","877de1458c534b138d8311dffef32d27","b6f9da8c7b114304825736f08d95ef83","8c6173d2290e403bb8608ff436b9ce10","32491a2e2b4246a08af989e0a9a79878","b413751041c8478b8f150bbd26fafe56","2abb1e4592d244c08db6ae1abd96f329","b9f841f3bc4b4899a543fe01d79a1fd2","06355a4c25e5406a85ca317bbe0e94f9","1a5045094cd14de0ab26fc14c10927d7","bc01847c749e49328eb85250bb3dd953","546c7c6c74b644a19b71e503db68493f","222e5ebc15264d99916e48bb24b9097d","06de1f64e28648c7809165e86e248fa7","e7f41d5295084ee9b1657b0afd5d4654","8f666b18f86e4935a6bb0cc3121c6f24","0aa08cdcde784e48818988873eccb937","5680d790c7834280a9c87a4e1c73ddd7","f7bbc4fe5564454bb5125ca946c77d3a","033c7c06e88540069e8dc3ae1a40a295","602b45da191743b8a40d91a38df3e469","c6e7df33081141c5aa921e64ca636356","78c1841e700a4776b2db54fddfc8eae0","20ab4a0f2c224ef0b3ea068fbf4ff3ad","3948917894a449dd93627ccc84f622fa","22f7df56de06429aade3aa3d93f7eb43","b2058b03f0644fa9b0ebee648d83dba5","c4e038bbec844d8893903b734bde796e","84c75fd8686c4aadb6014d3eb5917a75","7d3ce829858d46128c90e4689ef23f2a","9feffa25bbe64123b8aa8548bbc4da4e","e9f50bb606b2407da8779967e07ebebb","73e3dc1ea7ee4fae957201519e059e30","b4a44981a04346b384820936155f4f14","673d988d4ef14400899ac7f7e4160aa1","3f31177447954351b730016b370480ad","5afd59d39d6b4327b758e6f22d6553ee","87ca1969bc6e4e3aab83270fd168f7c8","0d8c815dd5d24860ae2deb89462ec31d","6710babe057446d0999b5d1beea65c29","247c4a8170ec4e78ad40dee8f7402f11","0d856956ccf347e8bd40053a3bbbc7fd","907ffde14fad4d40973a90999d3218b8","dd439fc0866941148aedbe493ba1a00b","11a1c25d7fbd4ba7ac1a8a4b0ee74952","40484f5ceae1431f9186af93590c1886","0f21ff43af7649838211f42a64d524b8","f500b69df2404bf68571bf8589a9689f","573d0209bdc04ce89c85738c7019838e","90941f7fed3747e6afb1f729ba70bf61","1f9988fb555f49419a6fe1349a3445c3","ea807c2e37384a2a8d3037a391b8540f","cfd89566b0694812a462b2a4aec248d8","f7ad3757eab44982806d95d7bb171820","258239f2217942e4b931ecc592489acc","567e9be41ae24c238dbf3f1658844769","e7c2dc0e76994728aed456d041f3b270","856991b6825f4608912fd0991b36d1df","1941150119df403cb08e09fcd686dcec","4fe635b110ae4bc3aa38672ded4a5cd5","83a2e7956e72425d87cdccf04cf631f3","58dd7d6182264a3d939077d937c4cbf8","6037fb0f8daf4d34860830ccee1b4329","eee007e13a054d67a05a6a4ae6b4823c","f84b3e9eaac14d75800b8bc27d6c48d0","213bf50f085240f7a22c7e4770cb056d","6b29f6536d814f6dab46cb67a44da263","63ecf3afaebb4b9da54a8c5c310bcf7e","a25b5d535a5d40ecab3d315a7027f170","6b6528b5ee2f4b2f9a3a6b1212536366","b904ab29e9004ad6bad22d8beab9b36a","332c270dd1f448ea9ccb000a1fc11b8f","31df1feace324aceb64bdfa2b9dbab2e","600a11007e944d5c9dd54317100bee7e","5a9c8507a668432c8deedac2290979b0","2a42457d6b6b41cda8159aea9db0e706","113822b5f9694d2e89018fefc92ae632","f70ac3e6c6714c63b9dfc4266f1f05b9","b5a02fd843f343ba9c4f530e3577a586","630556acc6cc436b9a8fb78b0a76f23a","05a420139c3944a9a9616d90867a61ea","77a3f18c6520445f922f73bb7e8be1ae","54303d7820464dac8319cbf94949efa0","6980009402084edbb10a7ca856672de2","aa38051f32114a4eab9ed1c945ac0672","3b3bb9f85b4d40278431b605a410fb73","2553ae13f47249dbad99adabcefe7111","7689980e9c4c4254865b61fd4a8f2e03","e319251b20fb4732afdfcb2f1ae084c0","8be1d30893b74e5190a0c1c32967d5e1","21d28b1f0be74a4895458cfa5f9b2eea","af4e052cbe6041fdb10f9d29cf4f8831","f395d9913a3347f98bc2e1753272a7cf","a403ad8af45444a596177802194f84d9","63b1c789bb514bd9bb3be72ed399fc04","8337430a619a4dfd8cfd1927114bdc53","05d4c5de974e49da8b1e0968c520c56e","116a0db5397c44f99f2b4916a439b99d","030c12f6865b4c329323f3856b0614f9","d2a83cd1b20f417b8dfec33944f04d59","097c35e4f7e146779d4b0fc2116b1a48","5a4cf313faf14c5593d24bbe0918f134","3fa6553f6ef44c61b3a0a50467f287b5","1036a0bf49e94ef59e73db5ca46ace27","4f082adfc4f145e59af2f60e52189010","5031fb3440fb41b6ae79c47c4ccae8e7","8b297d633fd746c596adca246a92c6d6","7e9065525bda4d07925c70041cadbb43","de3f2371eadc451fb5b23cbf5dc54341","95f83995ef154dcf8e5c54630211af09","0ad9c0d825ee455fbdd14a6128e661fa","1aaec25f3f8c4645843ef89f172b246f","a0c916a3ff884e74abd2c07da74b3bf3","14ff06fcb82f4c898ec7b4828edb8993","3df8e595688746a49c529121e5ec14c1","5470884b2cfd4d77994ffd4a2cb5926c","ed2ab8ca836a4d24b950e6d4d8538134","93cf721b4b4e4a13a151bd8c102ae5d7","1cf9235c23f34f38bb401b6f1b5c368c","834469d366ef426c97bbf1f5c282cebc","dbf7597adef74d369750e6cf6fe6a073","64834fd572da4b0bafa5ea42e03e02d4","c4ebfc2748ad49d4bbec3dee027e7bfe","b43a0f0b5d314eb091c0fe5b20772046","693cce5d69724a488c20494ea6ef0103","02f2406ac792413597b1c95113354f70","02001d2c16a64aa7b858939c760ee238","07f3bc307c46466d8a5031de0eea951b","115111a9c1d747e3833be819d02e6315","26a87947e701427a837a00cf8e97c3d6","c02bc35b73cd47f5b364e0c865e711fb","a61aec91787241d5a5d673e2be6af3ab","b592c48df84a40dd861e55ad3156e625","589fd522bd7541d8b774e2ef7766f4e9","48db43f14fea49f985f784589045aec3","3e62a00838bb42c8a63a91fa272a33cd","d80c9a8063d64712a51d49f0d06a5dc5","1a5ab24a484e4d8984102fe9933ce47e","6df2aa00f03a4206b9ff1a940eec14a3","d756e905c6de490d9f5b620b215a23be","3b9c456eddee4f258766be902e2bbb9c","5b0ab8780efa4da5979a39275a17e35f","490f511eafd24468a4764fcfa2286f4d","f38f2ea972b54778bd1f4a63f2c791de","c1f059c9906143ff8342e486d2b25e12","5ff8a1c7fb8744899de90d2d1cf2e2b5","4a2b83695bd2478f8e907e367e709a27","ad1fa25aa6624b9ebc1d661bb62a95bb","ce293de3d4dd4034959e48ec6edbb9af","8fbc321a996a45d493c9f27bca8b6335","3c1a3b8e65ee46ac90e980ea4b87a535","d9d5e76c54a446fab57d195b39acc36b","db2a52a9ecf448feb46f67e77f68c1ae","7f5f8139f43244b18def8e947bea2595","61980037b35842378fa7ffb732306d31","9c9ce20ae0894ebdb26d432559c1093a","cfee0bfddee841b6b8c0f364a46e103f","5ebf651e257141289349837be9f36622","d0434c62e0c64f8aa9dd8ea7fc6016ec"]},"executionInfo":{"elapsed":2801,"status":"ok","timestamp":1666244438355,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"ESygDI-J96YR","outputId":"b99f938b-b543-4bdd-e390-5d8488508ccb"},"outputs":[{"output_type":"stream","name":"stdout","text":["           "]},{"output_type":"display_data","data":{"text/plain":["#0:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cc7d68291442e3883b264c57f235d4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#1:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563bd3b228144cdf9f9cacfa1ef5bda3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#2:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2894677b67a4eeaa13d3aea2f1370d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["#3:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c57122d834594856b0ee3c384b925406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["#4:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e558ced6ebcc46c3937d0413f78304dd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#5:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd47314ec2441d491641609e46996ff"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#6:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53ce5c73b7f44ea3a99985bb45260905"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#7:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67cb95ffe32d48b6b594b4a652e42857"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["          "]},{"output_type":"display_data","data":{"text/plain":["#0:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd11182a4bf4053a54e77965db7bc9d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#1:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34449fb324244c89a3014c274845117d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#2:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b413751041c8478b8f150bbd26fafe56"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#3:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa08cdcde784e48818988873eccb937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["#4:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e038bbec844d8893903b734bde796e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  "]},{"output_type":"display_data","data":{"text/plain":["#5:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d8c815dd5d24860ae2deb89462ec31d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#6:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90941f7fed3747e6afb1f729ba70bf61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["#7:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83a2e7956e72425d87cdccf04cf631f3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["          "]},{"output_type":"display_data","data":{"text/plain":["#0:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332c270dd1f448ea9ccb000a1fc11b8f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#1:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54303d7820464dac8319cbf94949efa0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#2:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a403ad8af45444a596177802194f84d9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" "]},{"output_type":"display_data","data":{"text/plain":["#3:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f082adfc4f145e59af2f60e52189010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["#4:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5470884b2cfd4d77994ffd4a2cb5926c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   "]},{"output_type":"display_data","data":{"text/plain":["#5:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02001d2c16a64aa7b858939c760ee238"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["#6:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a5ab24a484e4d8984102fe9933ce47e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["#7:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce293de3d4dd4034959e48ec6edbb9af"}},"metadata":{}}],"source":["# cola = cola.map(lambda data: tokenizer(data[\"sentence\"], padding=True, max_length = CFG[\"max_length\"], truncation = True, return_token_type_ids = False), batched = True, remove_columns = [\"sentence\", \"idx\"], num_proc = 8)\n","\n","#sst2\n","data = data.map(lambda data: tokenizer(data[\"sentence1\"], data[\"sentence2\"], padding=True, max_length = CFG[\"max_length\"], truncation = True, return_token_type_ids = False), batched = True, remove_columns = [\"sentence1\", \"idx\", \"sentence2\"], num_proc = 8)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666244438355,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"SmUuQQt53MKE"},"outputs":[],"source":["train = data[\"train\"]"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666244438356,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"WxS2Zq-j9Uw9"},"outputs":[],"source":["val = data[\"validation\"]"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666244438356,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"2ylZ0EqF9dtG"},"outputs":[],"source":["test = data[\"test\"]"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666244438356,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"WUfekJjTAHlo","outputId":"59ade8f1-1901-4f69-8e2b-f9e0c084ba44"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['label', 'input_ids', 'attention_mask'],\n","    num_rows: 635\n","})"]},"metadata":{},"execution_count":13}],"source":["train"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666244438357,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"pLB37gJZCr3V","outputId":"69f79430-1d8f-44f2-fb0e-49dbc45b8043"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['label', 'input_ids', 'attention_mask'],\n","    num_rows: 146\n","})"]},"metadata":{},"execution_count":14}],"source":["train.with_format(\"torch\", device = device)\n","val.with_format(\"torch\", device = device)\n","test.with_format(\"torch\", device = device)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1666244438357,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"w-Fzlf-7jzzb","outputId":"ccab8f2a-cd1b-45f4-ee32-238e2b6f0145"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: \"WANDB_API_KEY\"=\"d01c72067c0b354a28976e02d73dc7448c5499f2\"\n"]}],"source":["#PLEASE DON'T USE MY API KEY. I'M TRUSTING YOU.\n","\n","os.environ[\"XRT_TPU_CONFIG\"] = \"tpu_worker;0;10.0.0.2:8470\"\n","os.environ['WANDB_CONSOLE'] = 'off'\n","os.environ['WANDB_NOTEBOOK_NAME'] = 'Deep Supervision Research BERT + DS + CLS + AAM.ipynb'\n","os.environ['WANDB_API_KEY'] = \"d01c72067c0b354a28976e02d73dc7448c5499f2\"\n","%env \"WANDB_API_KEY\" \"d01c72067c0b354a28976e02d73dc7448c5499f2\""]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1399,"status":"ok","timestamp":1666244439751,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"AlrcXScDjzzb","outputId":"967fe3ca-6c94-48b9-ce57-d2e01d3339c1"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find Deep Supervision Research BERT + DS + CLS + AAM.ipynb.\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}],"source":["wandb.login()"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439751,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"9ao6YGGsHZfn"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439751,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"BKcj-aeRXLdx"},"outputs":[],"source":["class AccuracyTracker(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.num_correct = 0\n","        self.total = 0\n","        self.accuracy = 0\n","\n","    def update(self, val, n=1):\n","        self.num_correct += val\n","        self.total += n\n","        self.accuracy = self.num_correct / self.total"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439752,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"EIj0e07d8crC"},"outputs":[],"source":["#OBVIOUSLY, CHANGE THIS AS YOU NEED. USE SELF.LOG FOR ALL IMPORTANT METRICS\n","class Model(nn.Module):\n","    def __init__(self, config, vocab_length, data_loader_len):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.vocab_length = vocab_length\n","        self.base_model = AutoModel.from_pretrained(self.config[\"model_name\"], output_hidden_states = True)  \n","        self.base_model.resize_token_embeddings(vocab_length)\n","        self.base_model = self.base_model\n","\n","        self.fc = nn.Linear(self.base_model.config.hidden_size, 1)\n","        self.span = self.base_model.config.num_hidden_layers - self.config[\"layer_start\"] + 1\n","        \n","        self.config[\"layer_start\"] = self.base_model.config.num_hidden_layers * 2 // 3\n","\n","\n","        # self.output_weights = nn.Parameter(\n","        #         torch.tensor([1] * (self.base_model.config.num_hidden_layers + 1 - self.config[\"layer_start\"]), dtype=torch.float)\n","        # )\n","\n","        self.fcs = []\n","        \n","        self.dropout = torch.nn.Dropout(p=CFG[\"dropout\"])\n","\n","        if self.config[\"pooler\"] == \"deep\":\n","            for _ in range(self.span):\n","            # for _ in range(self.base_model.config.num_hidden_layers - self.base_model.config.num_hidden_layers // 2 + 1):\n","                layer = nn.Linear(self.base_model.config.hidden_size, 1)\n","                self._init_weights(layer)\n","                self.fcs.append(layer)\n","\n","        self._init_weights(self.fc)\n","        # self._init_weights(self.output_weights)\n","        self.data_loader_len = data_loader_len\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        elif isinstance(module, nn.parameter.Parameter):\n","            module.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n","        else:\n","            print(f\"Module of type {type(module)} cannot be initialized\")\n","\n","    def feature(self, inputs):\n","\n","        if self.config[\"pooler\"] == \"deep\":\n","            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n","        \n","            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"hidden_states\"]\n","\n","            x = torch.stack(x)\n","\n","            return x[self.config[\"layer_start\"]:, :, :, :]\n","            \n","        else:\n","            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n","        \n","            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"last_hidden_state\"]\n","\n","            return inputs[:, 0, :]\n","\n","            \n","\n","    def forward(self, inputs):\n","        \n","        features = self.feature(inputs)\n","\n","        if self.config[\"pooler\"] == \"deep\":\n","\n","            outputs = []\n","\n","            layers = []\n","\n","            for layer_num, layer in enumerate(features):\n","                \n","                layers.append(self.dropout(layer[:, 0, :]))\n","                \n","                outputs.append(self.fcs[layer_num](layers[-1]))\n","\n","            outputs = torch.stack(outputs)\n","\n","            layers = torch.stack(layers)\n","\n","            final_cls = torch.max(layers, dim = 0)[0]\n","\n","            # final_cls = torch.sum(self.output_weights.unsqueeze(dim = -1).unsqueeze(dim = -1).expand(layers.shape) * layers, dim = 0) / len(layers)\n","\n","            pred = self.fc(final_cls)\n","            \n","            return outputs, pred\n","\n","        return self.fc(features)"]},{"cell_type":"markdown","metadata":{"id":"kPPDgVLsjzzd"},"source":["# Packaging All The Above Functions into a Dataset"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439752,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"4lREk9P6jzzg"},"outputs":[],"source":["#CHANGE AS NEEDED. MOST OF THE TIME, PYTORCH'S DEFAULT COLLATOR IS ENOUGH.\n","class DataModule():\n","\n","    def __init__(self, config, train, val, test, collate_fn):\n","        self.config = config\n","        self.train, self.val, self.test = train, val, test\n","        self.collate_fn = collate_fn\n","\n","        \n","    def train_dataloader(self):\n","        train_loader = DataLoader(self.train, batch_size=self.config[\"batch_size\"], shuffle = True, collate_fn = self.collate_fn, pin_memory=True, num_workers = 8)\n","        # train_loader = DataLoader(self.train, batch_size=self.config[\"batch_size\"], shuffle = True, collate_fn = self.collate_fn)\n","        return train_loader\n","\n","    def val_dataloader(self):\n","        val_loader = DataLoader(self.val, batch_size = self.config[\"batch_size\"], collate_fn = self.collate_fn, pin_memory=True, num_workers = 8)\n","        # val_loader = DataLoader(self.val, batch_size = self.config[\"batch_size\"], collate_fn = self.collate_fn)\n","        return val_loader\n","\n","    def test_dataloader(self):\n","        test_loader = DataLoader(self.test, batch_size = self.config[\"batch_size\"], collate_fn = self.collate_fn, pin_memory=True,  num_workers = 8)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439753,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"DmHpjEfcwSC2"},"outputs":[],"source":["def configure_optimizers(config, model):\n","    \n","        if config[\"optimizer\"] == \"AdamW\":\n","            optimizer = AdamW(\n","                model.parameters(), \n","                weight_decay = config[\"weight_decay\"], \n","                lr=config[\"lr\"],\n","                correct_bias = True)\n","\n","        else:\n","            optimizer = AdamW(model.parameters(), weight_decay = config[\"weight_decay\"], lr=config[\"lr\"], correct_bias = True)\n","\n","        if config[\"scheduler\"] == \"cosine\":\n","            scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = config[\"num_warmup_steps\"], num_training_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"])\n","\n","        elif config['scheduler'] == \"one_cycle\":\n","            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = config[\"lr\"], pct_start = config[\"pct_start\"], total_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"])\n","            \n","        elif config['scheduler'] == \"linear\":\n","            scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"] * 0.1, num_training_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"])\n","            \n","        else:\n","            scheduler = None\n","            \n","        \n","        return optimizer, scheduler"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439753,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"wVPtW6szW6JV"},"outputs":[],"source":["def train_fn(config, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    model.train()\n","\n","    losses = [AverageMeter() for i in range(model.span + 1)]\n","    overall_loss = AverageMeter()\n","    train_accuracy = AccuracyTracker()\n","\n","    pbar = tqdm(train_loader, desc = f\"Training Loop Epoch: {epoch}\")\n","\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    latest_avg = None\n","        \n","    grad_norm = 0.0\n","\n","    latest_acc = 0.0\n","\n","    for batch_idx, batch in enumerate(pbar):\n","\n","        labels = batch.pop(\"labels\")\n","\n","        inputs = batch\n","\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","\n","        labels = labels.to(device).to(torch.float16)\n","\n","        batch_size = labels.size(0)\n","\n","        #First Train\n","        \n","        with torch.cuda.amp.autocast():\n","            outputs, y_hat = model(inputs)\n","\n","            train_loss = criterion(y_hat.flatten(), labels)\n","\n","            losses[-1].update(train_loss, batch_size)\n","\n","            for idx in range(len(outputs) - 1, -1, -1):\n","                aux_loss = config[\"aux_weight\"] * criterion(outputs[idx].flatten(), labels)\n","            \n","                losses[(idx)].update(aux_loss, batch_size)\n","            \n","                train_loss += aux_loss\n","\n","            overall_loss.update(train_loss, batch_size)\n","\n","            scaled_loss = train_loss / config[\"grad_accum\"]\n","        \n","\n","        probs = torch.sigmoid(y_hat)\n","\n","        num_correct = torch.sum((probs.flatten() > 0.5).to(int) == labels)\n","\n","        train_accuracy.update(num_correct, batch_size)\n","        \n","        scaler.scale(scaled_loss).backward()\n","        \n","\n","        if ((batch_idx + 1) % config[\"grad_accum\"] == 0) or (batch_idx + 1 == model.data_loader_len):\n","\n","            scaler.unscale_(optimizer)\n","\n","            # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_norm\"])\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            optimizer.zero_grad()\n","\n","            if not scheduler is None:\n","\n","                scheduler.step()\n","\n","\n","            latest_avg = f\"{overall_loss.avg:.4f}\"\n","\n","            latest_acc = f\"{train_accuracy.accuracy:.4f}\"\n","\n","\n","        text = f\"Epoch: {epoch} | Training_accuracy: {latest_acc} | Training Loss_avg: {latest_avg} | Training Loss_step: {overall_loss.val:.4f} | Learning Rate: {scheduler.get_last_lr()[0]:.4f} | Grad: {grad_norm:.4f}\" if not scheduler is None else f\"Epoch: {epoch} | Training Loss_avg: {latest_avg} | Training Loss_step: {overall_loss.val:.4f} | Grad: {grad_norm:.4f}\"\n","\n","        pbar.set_postfix_str(text)\n","\n","        pbar.refresh()\n","\n","        \n","        for idx, loss in enumerate(losses):\n","            if idx < len(losses) - 1:\n","                wandb.log({f\"Training Loss Step Layer {idx + config['layer_start']}\": loss.val})\n","                continue\n","            wandb.log({f\"Training Loss Step Output Layer\": loss.val})\n","\n","        wandb.log({f\"Training Accuracy Step\": train_accuracy.accuracy})\n","\n","    wandb.log({f\"Training Accuracy Epoch\": train_accuracy.accuracy})\n","\n","    wandb.log({f\"Overall Training Loss Epoch\": overall_loss.avg})\n","\n","    return overall_loss.avg"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439753,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"SJhNb7m7XEBn"},"outputs":[],"source":["def valid_fn(config, valid_loader, model, criterion, device, epoch):\n","    model.eval()\n","    \n","    losses = [AverageMeter() for i in range(model.span + 1)]\n","    overall_loss = AverageMeter()\n","    accuracy = AccuracyTracker()\n","\n","    #A MANUAL LOOP IS NEEDED HERE SINCE TRAINER FUNCTIONS DON'T GIVE YOU ACCESS TO MODEL PREDICTIONS\n","\n","    pbar = tqdm(valid_loader, desc = f\"Validation Loop Epoch: {epoch}\")\n","    for batch_idx, batch in enumerate(pbar):\n","        \n","        labels = batch.pop(\"labels\")\n","\n","        inputs = batch\n","\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","\n","        labels = labels.to(device).to(torch.float16)\n","\n","        batch_size = labels.size(0)\n","\n","        model = model.to(device)\n","        \n","\n","        with torch.no_grad():\n","            outputs, y_hat = model(inputs)\n","\n","            val_loss = criterion(y_hat.flatten(), labels)\n","\n","            losses[-1].update(val_loss, batch_size)\n","\n","            for idx in range(len(outputs) - 1, -1, -1):\n","                aux_loss = config[\"aux_weight\"] * criterion(outputs[idx].flatten(), labels)\n","                losses[idx].update(aux_loss, batch_size)\n","                val_loss += aux_loss\n","\n","        overall_loss.update(val_loss.item(), batch_size)\n","\n","        probs = torch.sigmoid(y_hat)\n","\n","        num_correct = torch.sum((probs.flatten() > 0.5).to(int) == labels)\n","        accuracy.update(num_correct, batch_size)\n","\n","        # pbar.set_postfix_str(f\"Epoch: {epoch} | Validation Loss_avg: {losses.avg:.4f} | Validation Loss_step: {losses.val:.4f}\")\n","        pbar.set_postfix_str(f\"Epoch: {epoch} | Validation Loss_avg: {overall_loss.avg:.4f} | Validation_accuracy_step: {accuracy.accuracy}\")\n","    \n","    for idx, loss in enumerate(losses):\n","        if idx < len(losses) - 1:\n","            wandb.log({f\"Validation Loss Epoch Layer {idx + config['layer_start']}\": loss.avg})\n","            continue\n","        wandb.log({f\"Validation Loss Epoch Output Layer\": loss.avg})\n","\n","    wandb.log({\"Overall Validation Loss Epoch\": overall_loss.avg})\n","\n","    wandb.log({f\"Validation Accuracy Epoch\": accuracy.accuracy})\n","\n","    return overall_loss.avg, accuracy.accuracy"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439754,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"zfC3gU9jxwA9"},"outputs":[],"source":["def test_fn(config, test_loader, model, criterion, device, checkpoint, class_names):\n","    with torch.no_grad():\n","        losses = AverageMeter()\n","        accuracy = AccuracyTracker()\n","\n","        # PUT YOUR CUSTOM GRAPHS HERE\n","        \n","        # LOAD BEST MODEL CHECKPOINT\n","\n","        saved = torch.load(checkpoint)\n","        model.load_state_dict(saved[\"model_state_dict\"])\n","        model.eval()\n","\n","\n","        #A MANUAL LOOP IS NEEDED HERE SINCE TRAINER FUNCTIONS DON'T GIVE YOU ACCESS TO MODEL PREDICTIONS\n","\n","        pbar = tqdm(test_loader, desc = f\"Getting Test Predictions\")\n","        for batch_idx, batch in enumerate(pbar):\n","            labels = batch.pop(\"labels\")\n","\n","            inputs = batch\n","\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","\n","            labels = labels.to(device).to(torch.float16)\n","\n","            batch_size = labels.size(0)\n","\n","            model = model.to(device)\n","            \n","            outputs, y_hat = model(inputs)\n","\n","            val_loss = criterion(y_hat.flatten(), labels)\n","\n","            for idx, output in enumerate(outputs[:-1]):\n","                val_loss += config[\"aux_weight\"] + criterion(output.flatten(), labels)\n","\n","            probs = torch.sigmoid(y_hat)\n","\n","            num_correct = torch.sum((probs.flatten() > 0.5).to(int) == labels)\n","\n","            losses.update(val_loss, batch_size)\n","\n","            accuracy.update(num_correct, batch_size)\n","\n","            #LOGGING SPECIFIC THINGS HERE #############################\n","\n","    \n","        #LOGGING OOF PERFORMANCE\n","        wandb.log({f\"Validation Accuracy\": accuracy.accuracy})\n","\n","        print(f\"Validation Accuracy: {accuracy.accuracy}\")\n","\n","        return losses.avg"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666244439754,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"-dqlGGJdqUS0"},"outputs":[],"source":["class ModelTracker():\n","    def __init__(self, patience, base_path, model, path, optimizer, scheduler, mode = \"maximize\", metric_name = \"accuracy\"):\n","        self.patience = patience\n","        self.mode = mode\n","        self.missed = 0\n","        self.path = path\n","        self.model = model\n","        self.base_path = base_path\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.metric = float(\"-inf\") if self.mode == \"maximize\" else float(\"inf\")\n","        self.metric_name = metric_name\n","\n","    def update(self, value, epoch):\n","        if self.mode == \"maximize\":\n","            if value > self.metric:\n","                print(f\"Validation {self.metric_name} rose from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n","                self.metric = value\n","                \n","                torch.save({\n","                    \"epoch\": epoch, \n","                    \"model_state_dict\": self.model.state_dict(), \n","                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n","                    \"accuracy\": self.metric,\n","                    \"scheduler\": self.scheduler.state_dict()\n","                }, f\"{self.base_path}/{self.path}\")\n","\n","                print(f\"Saved to model to {self.base_path}/{self.path}!\")\n","\n","                self.missed = 0\n","\n","            else:\n","                print(f\"Validation {self.metric_name} fell from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n","                print(f\"Model did not improve on epoch {epoch}\")\n","                self.missed += 1\n","        else:\n","            if value < self.metric:\n","                print(f\"Validation {self.metric_name} fell from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n","                self.metric = value\n","                \n","                torch.save({\n","                    \"epoch\": epoch, \n","                    \"model_state_dict\": self.model.state_dict(), \n","                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n","                    \"loss\": self.metric,\n","                    \"scheduler\": self.scheduler.state_dict()\n","                }, f\"{self.base_path}/{self.path}\")\n","\n","                self.missed = 0\n","\n","                print(f\"Saved to model to {self.base_path}/{self.path}!\")\n","\n","            else:\n","                print(f\"Validation {self.metric_name} rose from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n","                print(f\"Model did not improve on epoch {epoch}\")\n","                self.missed += 1\n","\n","    def get_full_path(self):\n","        return f\"{self.base_path}/{self.path}\"\n","        \n","    def check_improvement(self):\n","        return (self.missed < self.patience if self.mode == \"maximize\" else self.missed > self.patience) and (self.missed < self.patience)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1666244439998,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"ruzVJ42ujzzj"},"outputs":[],"source":["def train_loop(train, val, test, data_collator, config, device, weights=None, base_path = \"./\"):\n","    for seed in config[\"seed\"]:\n","        seed_everything(seed)\n","        classes = [\"negative\", \"positive\"]\n","\n","        wandb.init(project=\"Auxillary BERT Research\", entity = \"goggins\", group = config[\"dataset\"], config = config, job_type = f\"{config['model_name']} {config['type']}\", save_code = True, reinit = True, name = f\"Seed {seed}\")\n","\n","        criterion = torch.nn.BCEWithLogitsLoss()\n","\n","        validation_criterion = torch.nn.BCEWithLogitsLoss()\n","\n","        dataset = DataModule(config, train, val, test, data_collator)\n","\n","        train_loader = dataset.train_dataloader()\n","\n","        val_loader = dataset.val_dataloader()\n","\n","        model = Model(config, len(config[\"tokenizer\"]), len(train_loader))\n","        \n","        model = model.to(device)\n","\n","        for i, fc in enumerate(model.fcs):\n","            model.fcs[i] = fc.to(device)\n","\n","        optimizer, scheduler = configure_optimizers(config, model)\n","\n","        tracker = ModelTracker(config[\"patience\"], base_path, model, f\"seed-{seed}.pt\", optimizer, scheduler)\n","\n","        for epoch in range(config[\"epochs\"]):\n","\n","            train_loss = train_fn(config, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","            val_loss, val_accuracy = valid_fn(config, val_loader, model, validation_criterion, device, epoch)\n","\n","            tracker.update(val_accuracy, epoch)\n","\n","            if not tracker.check_improvement():\n","                print(f\"Stopping the model at epoch {epoch} since the model did not improve!\")\n","                break\n","\n","\n","        checkpoint = tracker.get_full_path()\n","\n","        test_fn(config, val_loader, model, validation_criterion, device, checkpoint, classes)\n","\n","        del dataset, model\n","\n","        gc.collect()\n","\n","        torch.cuda.empty_cache()\n","        \n","        wandb.finish()\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1666244440007,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"eGhABbNI2Yz_"},"outputs":[],"source":["device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1666244440007,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"},"user_tz":420},"id":"YXBTlcHPkUGD","outputId":"47e42ff6-65dc-4b66-eeb4-e428fb39a5cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Automatic pdb calling has been turned ON\n"]}],"source":["%pdb"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["eb848d61ef80497fbff3e4dd14c1f7bc","46f98c3cf0a04204bf7a0e69abd6e46a","5c8fe20fc0f9462191b3f7e34ae36362","74fc4846390a4f61ba0f26874085e116","0fed2ee156b741368a45f81fdc3875ad","e025dfc7191f4e8dba38afe26b5811e9","f1de8a4467a6496c9225fd739c28224f","d092278dbe104976833ad038400c4266","4e5ba414c6a74fd29a49dfe3889a1249","739a790535d04a1ab2e80326c18a9219","b49f4a06adb548578e9f7565be49b46e","52c81e558f8b4e929f53d8ddad56f3fc","689dcd81ff8b410ea453f433568e9d2a","61c3af61d07e415b8ea2b3721a69eca0","b1593a6be59c4594aa06ab5d0a3ab7d8","4f12d745ef1b4844b8d798ba48c17938","b09dd9ff585e4ddf805591ae1643201d","9c914155f4284b2cb3ac671836d47cde","cb72a5e365224f4f9f1120cc85898fb1","4bd9d5f54de34f4fb79c221043da5d3c","74837c56b5c641eaa32d0600b57a0bc2","76db22ef57ba493080ab0f40434cb29f","8eab30c108584766bec300fe942b28ab","92a41a50a18b45d9ad9234f7fd705a8f","1701eb13a7f8467aa7e2d494e5fe9edd","75b4f122b7d04140a7e9d97f3825c3cf","c3d25412508c4b7d9b8818cce7b182ec","b7cc731d448b4315aa4519a68464db45","5f6b62382f2944599e80f6c543c84a63","0f287f5730cd4675b05b7bb8f33065f0","ce727309388a4901868b5d36892c0a25","4fa54939b6754947ad306b428485676e","468afdc3f2b84ca78f80a69fb8b1cb52","216700065fdc40d582e19446d3e32de4","b686c08581e94befb934327971fc65e2","c85978db29694ff0818e9c738f13c9d5","e6eff6e7cc2c46f7aca69ff18df4e9e0","8e602a229c0640e68d91c6f66b864246","08f598503db240c2855981aece2c5cd3","c5b4feb4c3f24dbbadec7e8b0e61e5f6","784ab70369fb47d595cab5c4de2cf3de","bd46cb32a6254d0bafba3824e42d0de0","ad69d2a341bd4892941cd49276c04f23"]},"id":"i9jBz0mZUXLZ","outputId":"20581b8d-3593-4d27-b3b6-c3040ce1595e","executionInfo":{"status":"ok","timestamp":1666244687102,"user_tz":420,"elapsed":247109,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221020_054039-2pqipqyv</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/2pqipqyv\" target=\"_blank\">Seed 42</a></strong> to <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb848d61ef80497fbff3e4dd14c1f7bc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","Training Loop Epoch: 0:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","Training Loop Epoch: 0: 100%|██████████| 20/20 [00:04<00:00,  4.04it/s, Epoch: 0 | Training_accuracy: 0.5039 | Training Loss_avg: 2.5225 | Training Loss_step: 2.4825 | Learning Rate: 0.0000 | Grad: 3.8356]\n","Validation Loop Epoch: 0:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 0: 100%|██████████| 3/3 [00:00<00:00,  6.31it/s, Epoch: 0 | Validation Loss_avg: 2.4331 | Validation_accuracy_step: 0.5352112650871277]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy rose from -inf to 0.5352 on epoch 0\n","Saved to model to .//seed-42.pt!\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 1:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 1: 100%|██████████| 20/20 [00:02<00:00,  9.63it/s, Epoch: 1 | Training_accuracy: 0.4992 | Training Loss_avg: 2.4774 | Training Loss_step: 2.4625 | Learning Rate: 0.0000 | Grad: 3.6132]\n","Validation Loop Epoch: 1:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 1: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s, Epoch: 1 | Validation Loss_avg: 2.4498 | Validation_accuracy_step: 0.2957746386528015]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.2958 on epoch 1\n","Model did not improve on epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 2:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 2: 100%|██████████| 20/20 [00:02<00:00,  9.54it/s, Epoch: 2 | Training_accuracy: 0.5024 | Training Loss_avg: 2.4529 | Training Loss_step: 2.4869 | Learning Rate: 0.0000 | Grad: inf]\n","Validation Loop Epoch: 2:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 2: 100%|██████████| 3/3 [00:00<00:00,  6.32it/s, Epoch: 2 | Validation Loss_avg: 2.4600 | Validation_accuracy_step: 0.2816901206970215]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.2817 on epoch 2\n","Model did not improve on epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 3:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 3: 100%|██████████| 20/20 [00:02<00:00,  9.60it/s, Epoch: 3 | Training_accuracy: 0.5102 | Training Loss_avg: 2.4479 | Training Loss_step: 2.4714 | Learning Rate: 0.0000 | Grad: 2.5744]\n","Validation Loop Epoch: 3:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 3: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s, Epoch: 3 | Validation Loss_avg: 2.4776 | Validation_accuracy_step: 0.22535210847854614]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.2254 on epoch 3\n","Model did not improve on epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 4:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 4: 100%|██████████| 20/20 [00:02<00:00,  9.53it/s, Epoch: 4 | Training_accuracy: 0.5024 | Training Loss_avg: 2.4423 | Training Loss_step: 2.3860 | Learning Rate: 0.0000 | Grad: 2.4823]\n","Validation Loop Epoch: 4:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 4: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s, Epoch: 4 | Validation Loss_avg: 2.5030 | Validation_accuracy_step: 0.2816901206970215]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.2817 on epoch 4\n","Model did not improve on epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 5:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 5: 100%|██████████| 20/20 [00:02<00:00,  9.43it/s, Epoch: 5 | Training_accuracy: 0.4929 | Training Loss_avg: 2.4565 | Training Loss_step: 2.4773 | Learning Rate: 0.0000 | Grad: 3.3662]\n","Validation Loop Epoch: 5:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 5: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s, Epoch: 5 | Validation Loss_avg: 2.4761 | Validation_accuracy_step: 0.35211268067359924]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.3521 on epoch 5\n","Model did not improve on epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 6:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 6: 100%|██████████| 20/20 [00:02<00:00,  9.34it/s, Epoch: 6 | Training_accuracy: 0.5213 | Training Loss_avg: 2.4391 | Training Loss_step: 2.4336 | Learning Rate: 0.0000 | Grad: 2.9631]\n","Validation Loop Epoch: 6:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 6: 100%|██████████| 3/3 [00:00<00:00,  6.30it/s, Epoch: 6 | Validation Loss_avg: 2.4990 | Validation_accuracy_step: 0.19718310236930847]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1972 on epoch 6\n","Model did not improve on epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 7:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 7: 100%|██████████| 20/20 [00:02<00:00,  9.48it/s, Epoch: 7 | Training_accuracy: 0.5260 | Training Loss_avg: 2.4358 | Training Loss_step: 2.4271 | Learning Rate: 0.0000 | Grad: 2.3862]\n","Validation Loop Epoch: 7:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 7: 100%|██████████| 3/3 [00:00<00:00,  6.28it/s, Epoch: 7 | Validation Loss_avg: 2.5222 | Validation_accuracy_step: 0.18309858441352844]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1831 on epoch 7\n","Model did not improve on epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 8:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 8: 100%|██████████| 20/20 [00:02<00:00,  9.54it/s, Epoch: 8 | Training_accuracy: 0.4756 | Training Loss_avg: 2.4433 | Training Loss_step: 2.4467 | Learning Rate: 0.0000 | Grad: 2.4619]\n","Validation Loop Epoch: 8:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 8: 100%|██████████| 3/3 [00:00<00:00,  6.19it/s, Epoch: 8 | Validation Loss_avg: 2.5360 | Validation_accuracy_step: 0.18309858441352844]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1831 on epoch 8\n","Model did not improve on epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 9:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 9: 100%|██████████| 20/20 [00:02<00:00,  9.43it/s, Epoch: 9 | Training_accuracy: 0.5354 | Training Loss_avg: 2.4322 | Training Loss_step: 2.4320 | Learning Rate: 0.0000 | Grad: 2.4963]\n","Validation Loop Epoch: 9:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 9: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s, Epoch: 9 | Validation Loss_avg: 2.5466 | Validation_accuracy_step: 0.15492957830429077]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1549 on epoch 9\n","Model did not improve on epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 10:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 10: 100%|██████████| 20/20 [00:02<00:00,  9.39it/s, Epoch: 10 | Training_accuracy: 0.5213 | Training Loss_avg: 2.4305 | Training Loss_step: 2.4545 | Learning Rate: 0.0000 | Grad: 3.4035]\n","Validation Loop Epoch: 10:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 10: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s, Epoch: 10 | Validation Loss_avg: 2.5704 | Validation_accuracy_step: 0.15492957830429077]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1549 on epoch 10\n","Model did not improve on epoch 10\n","Stopping the model at epoch 10 since the model did not improve!\n"]},{"output_type":"stream","name":"stderr","text":["\rGetting Test Predictions:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Getting Test Predictions: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5352112650871277\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.446 MB of 0.446 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c81e558f8b4e929f53d8ddad56f3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>█▅▃▂▂▃▂▁▂▁▁</td></tr><tr><td>Overall Validation Loss Epoch</td><td>▁▂▂▃▅▃▄▆▆▇█</td></tr><tr><td>Training Accuracy Epoch</td><td>▄▄▄▅▄▃▆▇▁█▆</td></tr><tr><td>Training Accuracy Step</td><td>▅▆▆▅▁▃▄▅▆▆▆█▆▅▆▅▆▅▁▄▄▅▅▄▆▆█▇▇▄▄▄▄▇▆▆▅▇▇▆</td></tr><tr><td>Training Loss Step Layer 10</td><td>▆▆▅▇▅▄▄▅▄▁▅█▆▄▇▃▅▁▂▅▃▂▆▆▅▂▃▅▄▅▇▄▃▂▄▄▂▆▄▅</td></tr><tr><td>Training Loss Step Layer 11</td><td>▃▇▅▁█▅▆▄▇▃▅▇▆▄▅▅▄▆▆▆█▄▅▅▄▆▅▅▆▄▆▇▅▇▃▇▄▂▅▆</td></tr><tr><td>Training Loss Step Layer 12</td><td>▁▄▂▅▂▇▅▆▅▅▂▄▃▅▄▃█▃▅▃▆▆▄▇▁▅▁▄▄▅▆▄▃▄▄▄▆▃▁▄</td></tr><tr><td>Training Loss Step Layer 13</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss Step Layer 14</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss Step Layer 15</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss Step Layer 16</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss Step Layer 17</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss Step Layer 18</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss Step Layer 19</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Loss Step Layer 8</td><td>▆█▅▁▅▂▄▆▅▃▃▄▅▃▄▄▄▃▃▅▆▅▄▄▃▂▄▃▄▄▅▃▅▃▄▄▆▄▄▄</td></tr><tr><td>Training Loss Step Layer 9</td><td>▆▆▅▃█▁▄▄▅▃▆▄▄▅▆▆▄▅▅▅▃▄▂▇▄▇▄▃▄▆▅▄▆▄▆▄▆▅▄▄</td></tr><tr><td>Training Loss Step Output Layer</td><td>▇▇▅▂█▂▂▄▄▂▄▅▄▃▄▃▅▃▄▅▅▂▄▇▁▃▁▃▃▅▆▄▃▂▂▃▅▂▁▄</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Accuracy Epoch</td><td>█▄▃▂▃▅▂▂▂▁▁</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>▂▁▂▃▄▄▄▅▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>▁▁▂▃▄▂▃▅▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>▂▁▂▃▄▄▄▆▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 13</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 14</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 15</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 16</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 17</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 18</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 19</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>▁▆▃▂▇▆▄▇███</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>▁▂▂▅▄▄▆▆▄▅█</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>▁▃▃▄▅▄▅▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>2.43055</td></tr><tr><td>Overall Validation Loss Epoch</td><td>2.5704</td></tr><tr><td>Training Accuracy Epoch</td><td>0.52126</td></tr><tr><td>Training Accuracy Step</td><td>0.52126</td></tr><tr><td>Training Loss Step Layer 10</td><td>0.35932</td></tr><tr><td>Training Loss Step Layer 11</td><td>0.35271</td></tr><tr><td>Training Loss Step Layer 12</td><td>0.34928</td></tr><tr><td>Training Loss Step Layer 13</td><td>0</td></tr><tr><td>Training Loss Step Layer 14</td><td>0</td></tr><tr><td>Training Loss Step Layer 15</td><td>0</td></tr><tr><td>Training Loss Step Layer 16</td><td>0</td></tr><tr><td>Training Loss Step Layer 17</td><td>0</td></tr><tr><td>Training Loss Step Layer 18</td><td>0</td></tr><tr><td>Training Loss Step Layer 19</td><td>0</td></tr><tr><td>Training Loss Step Layer 8</td><td>0.35195</td></tr><tr><td>Training Loss Step Layer 9</td><td>0.3459</td></tr><tr><td>Training Loss Step Output Layer</td><td>2.45446</td></tr><tr><td>Validation Accuracy</td><td>0.53521</td></tr><tr><td>Validation Accuracy Epoch</td><td>0.15493</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>0.3584</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>0.37671</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>0.37183</td></tr><tr><td>Validation Loss Epoch Layer 13</td><td>0</td></tr><tr><td>Validation Loss Epoch Layer 14</td><td>0</td></tr><tr><td>Validation Loss Epoch Layer 15</td><td>0</td></tr><tr><td>Validation Loss Epoch Layer 16</td><td>0</td></tr><tr><td>Validation Loss Epoch Layer 17</td><td>0</td></tr><tr><td>Validation Loss Epoch Layer 18</td><td>0</td></tr><tr><td>Validation Loss Epoch Layer 19</td><td>0</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>0.35156</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>0.35059</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>0.76172</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">Seed 42</strong>: <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/2pqipqyv\" target=\"_blank\">https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/2pqipqyv</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221020_054039-2pqipqyv/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221020_054144-e2qnpj0j</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/e2qnpj0j\" target=\"_blank\">Seed 12</a></strong> to <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Training Loop Epoch: 0:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 0: 100%|██████████| 20/20 [00:02<00:00,  9.25it/s, Epoch: 0 | Training_accuracy: 0.5244 | Training Loss_avg: 2.5360 | Training Loss_step: 2.5457 | Learning Rate: 0.0000 | Grad: 3.6338]\n","Validation Loop Epoch: 0:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 0: 100%|██████████| 3/3 [00:00<00:00,  5.47it/s, Epoch: 0 | Validation Loss_avg: 2.4711 | Validation_accuracy_step: 0.43661969900131226]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy rose from -inf to 0.4366 on epoch 0\n","Saved to model to .//seed-12.pt!\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 1:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 1: 100%|██████████| 20/20 [00:02<00:00,  9.11it/s, Epoch: 1 | Training_accuracy: 0.4882 | Training Loss_avg: 2.4704 | Training Loss_step: 2.4187 | Learning Rate: 0.0000 | Grad: 3.1575]\n","Validation Loop Epoch: 1:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 1: 100%|██████████| 3/3 [00:00<00:00,  5.57it/s, Epoch: 1 | Validation Loss_avg: 2.4963 | Validation_accuracy_step: 0.4225352108478546]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.4225 on epoch 1\n","Model did not improve on epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 2:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 2: 100%|██████████| 20/20 [00:02<00:00,  9.14it/s, Epoch: 2 | Training_accuracy: 0.5150 | Training Loss_avg: 2.4763 | Training Loss_step: 2.4745 | Learning Rate: 0.0000 | Grad: 3.0656]\n","Validation Loop Epoch: 2:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 2: 100%|██████████| 3/3 [00:00<00:00,  5.68it/s, Epoch: 2 | Validation Loss_avg: 2.5101 | Validation_accuracy_step: 0.43661969900131226]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.4366 on epoch 2\n","Model did not improve on epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 3:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 3: 100%|██████████| 20/20 [00:02<00:00,  9.07it/s, Epoch: 3 | Training_accuracy: 0.5260 | Training Loss_avg: 2.4593 | Training Loss_step: 2.4810 | Learning Rate: 0.0000 | Grad: 3.7957]\n","Validation Loop Epoch: 3:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 3: 100%|██████████| 3/3 [00:00<00:00,  5.56it/s, Epoch: 3 | Validation Loss_avg: 2.5194 | Validation_accuracy_step: 0.43661969900131226]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.4366 on epoch 3\n","Model did not improve on epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 4:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 4: 100%|██████████| 20/20 [00:02<00:00,  9.19it/s, Epoch: 4 | Training_accuracy: 0.5197 | Training Loss_avg: 2.4591 | Training Loss_step: 2.4313 | Learning Rate: 0.0000 | Grad: 3.6349]\n","Validation Loop Epoch: 4:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 4: 100%|██████████| 3/3 [00:00<00:00,  5.67it/s, Epoch: 4 | Validation Loss_avg: 2.5534 | Validation_accuracy_step: 0.43661969900131226]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.4366 on epoch 4\n","Model did not improve on epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 5:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 5: 100%|██████████| 20/20 [00:02<00:00,  9.36it/s, Epoch: 5 | Training_accuracy: 0.5008 | Training Loss_avg: 2.4444 | Training Loss_step: 2.4684 | Learning Rate: 0.0000 | Grad: 2.4682]\n","Validation Loop Epoch: 5:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 5: 100%|██████████| 3/3 [00:00<00:00,  5.58it/s, Epoch: 5 | Validation Loss_avg: 2.5449 | Validation_accuracy_step: 0.43661969900131226]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.4366 on epoch 5\n","Model did not improve on epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 6:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 6: 100%|██████████| 20/20 [00:02<00:00,  9.24it/s, Epoch: 6 | Training_accuracy: 0.5087 | Training Loss_avg: 2.4494 | Training Loss_step: 2.4663 | Learning Rate: 0.0000 | Grad: 3.2224]\n","Validation Loop Epoch: 6:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 6: 100%|██████████| 3/3 [00:00<00:00,  5.56it/s, Epoch: 6 | Validation Loss_avg: 2.5722 | Validation_accuracy_step: 0.43661969900131226]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.4366 on epoch 6\n","Model did not improve on epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 7:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 7: 100%|██████████| 20/20 [00:02<00:00,  9.23it/s, Epoch: 7 | Training_accuracy: 0.4961 | Training Loss_avg: 2.4422 | Training Loss_step: 2.4164 | Learning Rate: 0.0000 | Grad: 2.6335]\n","Validation Loop Epoch: 7:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 7: 100%|██████████| 3/3 [00:00<00:00,  5.62it/s, Epoch: 7 | Validation Loss_avg: 2.6082 | Validation_accuracy_step: 0.43661969900131226]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.4366 on epoch 7\n","Model did not improve on epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 8:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 8: 100%|██████████| 20/20 [00:02<00:00,  9.22it/s, Epoch: 8 | Training_accuracy: 0.5197 | Training Loss_avg: 2.4415 | Training Loss_step: 2.5419 | Learning Rate: 0.0000 | Grad: 3.2269]\n","Validation Loop Epoch: 8:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 8: 100%|██████████| 3/3 [00:00<00:00,  5.64it/s, Epoch: 8 | Validation Loss_avg: 2.6147 | Validation_accuracy_step: 0.39436620473861694]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.3944 on epoch 8\n","Model did not improve on epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 9:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 9: 100%|██████████| 20/20 [00:02<00:00,  9.14it/s, Epoch: 9 | Training_accuracy: 0.5685 | Training Loss_avg: 2.4336 | Training Loss_step: 2.5136 | Learning Rate: 0.0000 | Grad: 2.8497]\n","Validation Loop Epoch: 9:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 9: 100%|██████████| 3/3 [00:00<00:00,  5.54it/s, Epoch: 9 | Validation Loss_avg: 2.6413 | Validation_accuracy_step: 0.39436620473861694]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.3944 on epoch 9\n","Model did not improve on epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 10:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 10: 100%|██████████| 20/20 [00:02<00:00,  9.16it/s, Epoch: 10 | Training_accuracy: 0.5638 | Training Loss_avg: 2.4301 | Training Loss_step: 2.4920 | Learning Rate: 0.0000 | Grad: 3.0204]\n","Validation Loop Epoch: 10:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 10: 100%|██████████| 3/3 [00:00<00:00,  5.59it/s, Epoch: 10 | Validation Loss_avg: 2.6423 | Validation_accuracy_step: 0.39436620473861694]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.4366 to 0.3944 on epoch 10\n","Model did not improve on epoch 10\n","Stopping the model at epoch 10 since the model did not improve!\n"]},{"output_type":"stream","name":"stderr","text":["\rGetting Test Predictions:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Getting Test Predictions: 100%|██████████| 3/3 [00:00<00:00,  5.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.43661969900131226\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.499 MB of 0.499 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd9d5f54de34f4fb79c221043da5d3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>█▄▄▃▃▂▂▂▂▁▁</td></tr><tr><td>Overall Validation Loss Epoch</td><td>▁▂▃▃▄▄▅▇▇██</td></tr><tr><td>Training Accuracy Epoch</td><td>▄▁▃▄▄▂▃▂▄██</td></tr><tr><td>Training Accuracy Step</td><td>▄▄▅▅▂▃▄▃▆▄▄▇▆▅▅▄▅▅▁▂▃▄▆▄▄▄▇▅▄▇▅▄▄▇▇▆█▇▇▆</td></tr><tr><td>Training Loss Step Layer 10</td><td>▃▂▄█▁▄▃▃▅▅▄▁▃▂▂▄▃▃▃▃▂▂▂▃▂▂▃▄▃▃▃▂▃▂▂▂▃▃▂▅</td></tr><tr><td>Training Loss Step Layer 11</td><td>▇▇▄▅▇▄▆▅▅▃▄▅▅▇▆▃▂▅█▃▅▅▅▇▅▄▂▇▂▄▂▆▃▅▃▇▃▁▆▅</td></tr><tr><td>Training Loss Step Layer 12</td><td>▇▇▃▅▂█▅▁▅▃▅▆▃▄▄▄▄▅▄▂▄▃▂▄▅▃▃▅▃▂▂▄▅▄▂▄▃▃▅▃</td></tr><tr><td>Training Loss Step Layer 8</td><td>▇▅█▆▇▅▂▃▅▄▃▄▆▄▄▄▅▁▅▅▃▄▆▄▅▅▄▅▂▃▅▅█▂▄▃▅▄▃▄</td></tr><tr><td>Training Loss Step Layer 9</td><td>▇▄▂▃▄█▃▅▂▁▁▄▃▆▄▄▅▁▅▄▂▃▂▃▄▃▄▆▄▃▄▄▄▂▄▄▃▃▃▅</td></tr><tr><td>Training Loss Step Output Layer</td><td>█▆▆▆▄▅▂▂▅▃▃▃▃▄▄▄▃▂▄▃▃▃▃▄▄▃▂▆▃▁▂▄▅▂▃▃▂▂▄▄</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Accuracy Epoch</td><td>█▆██████▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>▁▄▂▃▄▄▅▆▆██</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>▂▁▂▂▃▃▄▆▇██</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>▁▃▃▃▄▄▅▆▇██</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>▁█▄▃▅▄▆▆▆▇▆</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>▁▃▇▂▅▅▇▇▆██</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>▁▁▂▃▅▄▆█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>2.43011</td></tr><tr><td>Overall Validation Loss Epoch</td><td>2.64228</td></tr><tr><td>Training Accuracy Epoch</td><td>0.56378</td></tr><tr><td>Training Accuracy Step</td><td>0.56378</td></tr><tr><td>Training Loss Step Layer 10</td><td>0.37077</td></tr><tr><td>Training Loss Step Layer 11</td><td>0.35451</td></tr><tr><td>Training Loss Step Layer 12</td><td>0.3425</td></tr><tr><td>Training Loss Step Layer 8</td><td>0.35375</td></tr><tr><td>Training Loss Step Layer 9</td><td>0.36049</td></tr><tr><td>Training Loss Step Output Layer</td><td>2.49202</td></tr><tr><td>Validation Accuracy</td><td>0.43662</td></tr><tr><td>Validation Accuracy Epoch</td><td>0.39437</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>0.36572</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>0.3833</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>0.38306</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>0.35376</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>0.35034</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>0.80566</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">Seed 12</strong>: <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/e2qnpj0j\" target=\"_blank\">https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/e2qnpj0j</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221020_054144-e2qnpj0j/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221020_054225-3q41dt0b</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/3q41dt0b\" target=\"_blank\">Seed 48</a></strong> to <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Training Loop Epoch: 0:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 0: 100%|██████████| 20/20 [00:02<00:00,  9.19it/s, Epoch: 0 | Training_accuracy: 0.4614 | Training Loss_avg: 2.5073 | Training Loss_step: 2.4738 | Learning Rate: 0.0000 | Grad: 4.2702]\n","Validation Loop Epoch: 0:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 0: 100%|██████████| 3/3 [00:00<00:00,  5.60it/s, Epoch: 0 | Validation Loss_avg: 2.4225 | Validation_accuracy_step: 0.5492957830429077]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy rose from -inf to 0.5493 on epoch 0\n","Saved to model to .//seed-48.pt!\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 1:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 1: 100%|██████████| 20/20 [00:02<00:00,  8.95it/s, Epoch: 1 | Training_accuracy: 0.4772 | Training Loss_avg: 2.4785 | Training Loss_step: 2.4792 | Learning Rate: 0.0000 | Grad: 2.8246]\n","Validation Loop Epoch: 1:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 1: 100%|██████████| 3/3 [00:00<00:00,  5.46it/s, Epoch: 1 | Validation Loss_avg: 2.4231 | Validation_accuracy_step: 0.563380241394043]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy rose from 0.5493 to 0.5634 on epoch 1\n","Saved to model to .//seed-48.pt!\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 2:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 2: 100%|██████████| 20/20 [00:02<00:00,  8.89it/s, Epoch: 2 | Training_accuracy: 0.5039 | Training Loss_avg: 2.4657 | Training Loss_step: 2.5058 | Learning Rate: 0.0000 | Grad: 3.7532]\n","Validation Loop Epoch: 2:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 2: 100%|██████████| 3/3 [00:00<00:00,  5.46it/s, Epoch: 2 | Validation Loss_avg: 2.4529 | Validation_accuracy_step: 0.4647887349128723]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.4648 on epoch 2\n","Model did not improve on epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 3:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 3: 100%|██████████| 20/20 [00:02<00:00,  9.13it/s, Epoch: 3 | Training_accuracy: 0.5039 | Training Loss_avg: 2.4515 | Training Loss_step: 2.4827 | Learning Rate: 0.0000 | Grad: 2.3288]\n","Validation Loop Epoch: 3:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 3: 100%|██████████| 3/3 [00:00<00:00,  5.52it/s, Epoch: 3 | Validation Loss_avg: 2.4509 | Validation_accuracy_step: 0.563380241394043]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.5634 on epoch 3\n","Model did not improve on epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 4:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 4: 100%|██████████| 20/20 [00:02<00:00,  9.24it/s, Epoch: 4 | Training_accuracy: 0.5134 | Training Loss_avg: 2.4588 | Training Loss_step: 2.4527 | Learning Rate: 0.0000 | Grad: 2.9332]\n","Validation Loop Epoch: 4:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 4: 100%|██████████| 3/3 [00:00<00:00,  5.60it/s, Epoch: 4 | Validation Loss_avg: 2.4760 | Validation_accuracy_step: 0.5352112650871277]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.5352 on epoch 4\n","Model did not improve on epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 5:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 5: 100%|██████████| 20/20 [00:02<00:00,  9.19it/s, Epoch: 5 | Training_accuracy: 0.5402 | Training Loss_avg: 2.4479 | Training Loss_step: 2.4861 | Learning Rate: 0.0000 | Grad: 2.6045]\n","Validation Loop Epoch: 5:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 5: 100%|██████████| 3/3 [00:00<00:00,  5.54it/s, Epoch: 5 | Validation Loss_avg: 2.4997 | Validation_accuracy_step: 0.4225352108478546]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.4225 on epoch 5\n","Model did not improve on epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 6:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 6: 100%|██████████| 20/20 [00:02<00:00,  9.23it/s, Epoch: 6 | Training_accuracy: 0.5197 | Training Loss_avg: 2.4509 | Training Loss_step: 2.4837 | Learning Rate: 0.0000 | Grad: 2.6894]\n","Validation Loop Epoch: 6:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 6: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s, Epoch: 6 | Validation Loss_avg: 2.5053 | Validation_accuracy_step: 0.35211268067359924]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.3521 on epoch 6\n","Model did not improve on epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 7:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 7: 100%|██████████| 20/20 [00:02<00:00,  9.21it/s, Epoch: 7 | Training_accuracy: 0.5543 | Training Loss_avg: 2.4276 | Training Loss_step: 2.3994 | Learning Rate: 0.0000 | Grad: 2.6934]\n","Validation Loop Epoch: 7:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 7: 100%|██████████| 3/3 [00:00<00:00,  5.50it/s, Epoch: 7 | Validation Loss_avg: 2.5290 | Validation_accuracy_step: 0.2957746386528015]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.2958 on epoch 7\n","Model did not improve on epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 8:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 8: 100%|██████████| 20/20 [00:02<00:00,  9.10it/s, Epoch: 8 | Training_accuracy: 0.5181 | Training Loss_avg: 2.4444 | Training Loss_step: 2.4597 | Learning Rate: 0.0000 | Grad: 2.6878]\n","Validation Loop Epoch: 8:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 8: 100%|██████████| 3/3 [00:00<00:00,  5.52it/s, Epoch: 8 | Validation Loss_avg: 2.5572 | Validation_accuracy_step: 0.3239436447620392]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.3239 on epoch 8\n","Model did not improve on epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 9:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 9: 100%|██████████| 20/20 [00:02<00:00,  9.22it/s, Epoch: 9 | Training_accuracy: 0.5339 | Training Loss_avg: 2.4276 | Training Loss_step: 2.3731 | Learning Rate: 0.0000 | Grad: 3.2078]\n","Validation Loop Epoch: 9:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 9: 100%|██████████| 3/3 [00:00<00:00,  5.54it/s, Epoch: 9 | Validation Loss_avg: 2.5964 | Validation_accuracy_step: 0.19718310236930847]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.1972 on epoch 9\n","Model did not improve on epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 10:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 10: 100%|██████████| 20/20 [00:02<00:00,  9.22it/s, Epoch: 10 | Training_accuracy: 0.5465 | Training Loss_avg: 2.4327 | Training Loss_step: 2.4306 | Learning Rate: 0.0000 | Grad: 3.2885]\n","Validation Loop Epoch: 10:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 10: 100%|██████████| 3/3 [00:00<00:00,  5.55it/s, Epoch: 10 | Validation Loss_avg: 2.6269 | Validation_accuracy_step: 0.18309858441352844]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.1831 on epoch 10\n","Model did not improve on epoch 10\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 11:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 11: 100%|██████████| 20/20 [00:02<00:00,  9.18it/s, Epoch: 11 | Training_accuracy: 0.5276 | Training Loss_avg: 2.4171 | Training Loss_step: 2.4668 | Learning Rate: 0.0000 | Grad: 3.2271]\n","Validation Loop Epoch: 11:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 11: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s, Epoch: 11 | Validation Loss_avg: 2.6618 | Validation_accuracy_step: 0.1690140813589096]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5634 to 0.1690 on epoch 11\n","Model did not improve on epoch 11\n","Stopping the model at epoch 11 since the model did not improve!\n"]},{"output_type":"stream","name":"stderr","text":["\rGetting Test Predictions:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Getting Test Predictions: 100%|██████████| 3/3 [00:00<00:00,  5.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.563380241394043\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.553 MB of 0.553 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7cc731d448b4315aa4519a68464db45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>█▆▅▄▄▃▄▂▃▂▂▁</td></tr><tr><td>Overall Validation Loss Epoch</td><td>▁▁▂▂▃▃▃▄▅▆▇█</td></tr><tr><td>Training Accuracy Epoch</td><td>▁▂▄▄▅▇▅█▅▆▇▆</td></tr><tr><td>Training Accuracy Step</td><td>▁▁▁▁▂▃▂▅▄▃▃▄▅▄▄▃▄▁▆▅▃▆▅▇▇▆▆▆▅▄▆▆▅█▅▅▆▇▇▅</td></tr><tr><td>Training Loss Step Layer 10</td><td>▆██▆▇▄▅▅▅▃▃▅▂▄▁▄▅▃▂▅▅▃▆▄▅▆▂▇▁▄▅▃▆▂▆▄▅▅▅▂</td></tr><tr><td>Training Loss Step Layer 11</td><td>▄▄▄█▄▇▆▄▄▂▅▄▄▅▄▅▃▆▁▄▃▃▄▄▆▄▄▄▅▃▃▇▆▄▅▃▄▄▅▆</td></tr><tr><td>Training Loss Step Layer 12</td><td>▆▆▄▂▄▃▄▂▄▄▄▄█▄▄▆▄▂▅▄▄▄▂▃▄▁▂▃▃▄▄▃▆▂▅▄▅▃▃▄</td></tr><tr><td>Training Loss Step Layer 8</td><td>▄▂█▅▂▄▄▁▅▄▄▄▃▄▄▃▅▃▆▅▅▃▃▃▂▂▆▃▁▆▄▃▂▅▃▄▃▃▄▄</td></tr><tr><td>Training Loss Step Layer 9</td><td>▇▇█▂▄▁▄▄▂▃▂▄▂▅▄▃▄▂▄▂▃▃▅▃▂▁▃▃▃▂▃▅▃▄▃▄▁▂▄▂</td></tr><tr><td>Training Loss Step Output Layer</td><td>▇▇█▅▅▅▆▃▃▄▅▅▂▅▃▅▄▃▂▃▄▁▄▂▅▁▁▅▂▃▄▅▅▂▅▃▃▂▄▄</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Accuracy Epoch</td><td>██▆██▆▄▃▄▂▁▁</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>▁▂▂▂▃▄▄▅▆▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>▂▁▃▃▄▄▄▅▅▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>▁▁▂▂▂▃▃▄▄▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>▃▁▅▂▃▄▃▅▇▆▆█</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>▁▅▅▄▆▆▆▆█▇▆▇</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>▁▁▂▂▂▃▄▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>2.41714</td></tr><tr><td>Overall Validation Loss Epoch</td><td>2.66178</td></tr><tr><td>Training Accuracy Epoch</td><td>0.52756</td></tr><tr><td>Training Accuracy Step</td><td>0.52756</td></tr><tr><td>Training Loss Step Layer 10</td><td>0.33675</td></tr><tr><td>Training Loss Step Layer 11</td><td>0.36787</td></tr><tr><td>Training Loss Step Layer 12</td><td>0.35657</td></tr><tr><td>Training Loss Step Layer 8</td><td>0.34988</td></tr><tr><td>Training Loss Step Layer 9</td><td>0.34504</td></tr><tr><td>Training Loss Step Output Layer</td><td>2.4668</td></tr><tr><td>Validation Accuracy</td><td>0.56338</td></tr><tr><td>Validation Accuracy Epoch</td><td>0.16901</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>0.36597</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>0.38306</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>0.42505</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>0.35156</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>0.3457</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>0.79004</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">Seed 48</strong>: <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/3q41dt0b\" target=\"_blank\">https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/3q41dt0b</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221020_054225-3q41dt0b/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221020_054312-1o6a5hyu</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/1o6a5hyu\" target=\"_blank\">Seed 55</a></strong> to <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Training Loop Epoch: 0:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 0: 100%|██████████| 20/20 [00:02<00:00,  9.14it/s, Epoch: 0 | Training_accuracy: 0.4803 | Training Loss_avg: 2.5194 | Training Loss_step: 2.4774 | Learning Rate: 0.0000 | Grad: 4.9949]\n","Validation Loop Epoch: 0:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 0: 100%|██████████| 3/3 [00:00<00:00,  5.47it/s, Epoch: 0 | Validation Loss_avg: 2.4471 | Validation_accuracy_step: 0.4647887349128723]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy rose from -inf to 0.4648 on epoch 0\n","Saved to model to .//seed-55.pt!\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 1:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 1: 100%|██████████| 20/20 [00:02<00:00,  9.05it/s, Epoch: 1 | Training_accuracy: 0.4850 | Training Loss_avg: 2.4839 | Training Loss_step: 2.5390 | Learning Rate: 0.0000 | Grad: 4.1651]\n","Validation Loop Epoch: 1:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 1: 100%|██████████| 3/3 [00:00<00:00,  5.57it/s, Epoch: 1 | Validation Loss_avg: 2.4585 | Validation_accuracy_step: 0.5352112650871277]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy rose from 0.4648 to 0.5352 on epoch 1\n","Saved to model to .//seed-55.pt!\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 2:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 2: 100%|██████████| 20/20 [00:02<00:00,  9.07it/s, Epoch: 2 | Training_accuracy: 0.5307 | Training Loss_avg: 2.4650 | Training Loss_step: 2.5041 | Learning Rate: 0.0000 | Grad: 3.6473]\n","Validation Loop Epoch: 2:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 2: 100%|██████████| 3/3 [00:00<00:00,  5.38it/s, Epoch: 2 | Validation Loss_avg: 2.4765 | Validation_accuracy_step: 0.30985915660858154]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.3099 on epoch 2\n","Model did not improve on epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 3:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 3: 100%|██████████| 20/20 [00:02<00:00,  8.94it/s, Epoch: 3 | Training_accuracy: 0.5134 | Training Loss_avg: 2.4412 | Training Loss_step: 2.3724 | Learning Rate: 0.0000 | Grad: 2.5697]\n","Validation Loop Epoch: 3:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 3: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s, Epoch: 3 | Validation Loss_avg: 2.4858 | Validation_accuracy_step: 0.3802816867828369]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.3803 on epoch 3\n","Model did not improve on epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 4:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 4: 100%|██████████| 20/20 [00:02<00:00,  9.23it/s, Epoch: 4 | Training_accuracy: 0.5213 | Training Loss_avg: 2.4336 | Training Loss_step: 2.4537 | Learning Rate: 0.0000 | Grad: 2.8637]\n","Validation Loop Epoch: 4:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 4: 100%|██████████| 3/3 [00:00<00:00,  5.55it/s, Epoch: 4 | Validation Loss_avg: 2.4978 | Validation_accuracy_step: 0.3380281627178192]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.3380 on epoch 4\n","Model did not improve on epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 5:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 5: 100%|██████████| 20/20 [00:02<00:00,  9.16it/s, Epoch: 5 | Training_accuracy: 0.5134 | Training Loss_avg: 2.4469 | Training Loss_step: 2.5234 | Learning Rate: 0.0000 | Grad: 3.4698]\n","Validation Loop Epoch: 5:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 5: 100%|██████████| 3/3 [00:00<00:00,  5.59it/s, Epoch: 5 | Validation Loss_avg: 2.5214 | Validation_accuracy_step: 0.2112676054239273]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.2113 on epoch 5\n","Model did not improve on epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 6:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 6: 100%|██████████| 20/20 [00:02<00:00,  9.20it/s, Epoch: 6 | Training_accuracy: 0.4945 | Training Loss_avg: 2.4416 | Training Loss_step: 2.4053 | Learning Rate: 0.0000 | Grad: 3.2786]\n","Validation Loop Epoch: 6:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 6: 100%|██████████| 3/3 [00:00<00:00,  5.54it/s, Epoch: 6 | Validation Loss_avg: 2.5522 | Validation_accuracy_step: 0.23943661153316498]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.2394 on epoch 6\n","Model did not improve on epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 7:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 7: 100%|██████████| 20/20 [00:02<00:00,  9.19it/s, Epoch: 7 | Training_accuracy: 0.5228 | Training Loss_avg: 2.4325 | Training Loss_step: 2.4718 | Learning Rate: 0.0000 | Grad: 4.4086]\n","Validation Loop Epoch: 7:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 7: 100%|██████████| 3/3 [00:00<00:00,  5.54it/s, Epoch: 7 | Validation Loss_avg: 2.5697 | Validation_accuracy_step: 0.3239436447620392]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.3239 on epoch 7\n","Model did not improve on epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 8:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 8: 100%|██████████| 20/20 [00:02<00:00,  9.09it/s, Epoch: 8 | Training_accuracy: 0.5354 | Training Loss_avg: 2.4406 | Training Loss_step: 2.4945 | Learning Rate: 0.0000 | Grad: 3.3376]\n","Validation Loop Epoch: 8:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 8: 100%|██████████| 3/3 [00:00<00:00,  5.39it/s, Epoch: 8 | Validation Loss_avg: 2.5924 | Validation_accuracy_step: 0.19718310236930847]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1972 on epoch 8\n","Model did not improve on epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 9:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 9: 100%|██████████| 20/20 [00:02<00:00,  9.15it/s, Epoch: 9 | Training_accuracy: 0.5417 | Training Loss_avg: 2.4157 | Training Loss_step: 2.4136 | Learning Rate: 0.0000 | Grad: 3.1907]\n","Validation Loop Epoch: 9:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 9: 100%|██████████| 3/3 [00:00<00:00,  5.43it/s, Epoch: 9 | Validation Loss_avg: 2.6146 | Validation_accuracy_step: 0.15492957830429077]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1549 on epoch 9\n","Model did not improve on epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 10:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 10: 100%|██████████| 20/20 [00:02<00:00,  9.10it/s, Epoch: 10 | Training_accuracy: 0.5496 | Training Loss_avg: 2.4311 | Training Loss_step: 2.4579 | Learning Rate: 0.0000 | Grad: 3.0413]\n","Validation Loop Epoch: 10:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 10: 100%|██████████| 3/3 [00:00<00:00,  5.43it/s, Epoch: 10 | Validation Loss_avg: 2.6380 | Validation_accuracy_step: 0.1690140813589096]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1690 on epoch 10\n","Model did not improve on epoch 10\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 11:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 11: 100%|██████████| 20/20 [00:02<00:00,  9.13it/s, Epoch: 11 | Training_accuracy: 0.5071 | Training Loss_avg: 2.4266 | Training Loss_step: 2.4011 | Learning Rate: 0.0000 | Grad: 3.3415]\n","Validation Loop Epoch: 11:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 11: 100%|██████████| 3/3 [00:00<00:00,  5.38it/s, Epoch: 11 | Validation Loss_avg: 2.6635 | Validation_accuracy_step: 0.1690140813589096]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5352 to 0.1690 on epoch 11\n","Model did not improve on epoch 11\n","Stopping the model at epoch 11 since the model did not improve!\n"]},{"output_type":"stream","name":"stderr","text":["\rGetting Test Predictions:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Getting Test Predictions: 100%|██████████| 3/3 [00:00<00:00,  5.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5352112650871277\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>█▆▄▃▂▃▃▂▃▁▂▂</td></tr><tr><td>Overall Validation Loss Epoch</td><td>▁▁▂▂▃▃▄▅▆▆▇█</td></tr><tr><td>Training Accuracy Epoch</td><td>▁▁▆▄▅▄▂▅▇▇█▄</td></tr><tr><td>Training Accuracy Step</td><td>▇▅▄▃▄▃▃▄▇▆▁▃▃▄▅▅▅█▆▅▆▄▃▆▇▆▅▆▆▅▆▇▆▇▆▇▆▆▆▄</td></tr><tr><td>Training Loss Step Layer 10</td><td>▃▂▅▂▄▇▇▅▇▄▃▄▁▁▇▅▄▄▄▅▁▃▆▄▄▇▂█▃▄▁▂▄▅▆▇█▂▄▃</td></tr><tr><td>Training Loss Step Layer 11</td><td>▅█▅▆▄▂▂▄▄▃▅▄▂▄▅▃▅▄▁▇▄▄▃▄▂▅▂▂▂▃▂▁▁▂▃▄▃▃▅▂</td></tr><tr><td>Training Loss Step Layer 12</td><td>▅▇▃▄▄█▂▃▆▅▄▁▃▂▅▅▄▂▄▇▄▁▄▂▃▅▄▃▂▂▄▂▃▄▂▄▃▂▃▄</td></tr><tr><td>Training Loss Step Layer 8</td><td>█▃▅▁▂▄▃▃▄▅▆▂▄▃▄▃▄▄▄▄▂▃▄▃▄▃▄▄▅▃▄▄▄▂▂▃▄▃▄▃</td></tr><tr><td>Training Loss Step Layer 9</td><td>▆█▄▆▃▃▅▅▅▃▅▂▁▃▆▃▅▅▂▇▅▂▅▅▃▅▅▄▇▄▃▆▃▅▂▆▅▅▄▄</td></tr><tr><td>Training Loss Step Output Layer</td><td>▇▇▅▅▃▆▃▄▆▄▆▂▂▁▅▄▆▃▁█▃▂▅▃▁▇▅▄▃▂▂▃▁▂▁▄▄▂▃▂</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Accuracy Epoch</td><td>▇█▄▅▄▂▃▄▂▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>▁▁▂▃▃▃▅▄▅▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>▂▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>▁▁▂▂▂▃▅▆▆▇▇█</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>▁▄▃▃▃▃▄▅▅▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>▂▁▁▂▂▂▃▃▃▅▇█</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>▁▂▃▂▃▄▅▅▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>2.42656</td></tr><tr><td>Overall Validation Loss Epoch</td><td>2.66354</td></tr><tr><td>Training Accuracy Epoch</td><td>0.50709</td></tr><tr><td>Training Accuracy Step</td><td>0.50709</td></tr><tr><td>Training Loss Step Layer 10</td><td>0.34332</td></tr><tr><td>Training Loss Step Layer 11</td><td>0.33503</td></tr><tr><td>Training Loss Step Layer 12</td><td>0.35282</td></tr><tr><td>Training Loss Step Layer 8</td><td>0.34806</td></tr><tr><td>Training Loss Step Layer 9</td><td>0.34452</td></tr><tr><td>Training Loss Step Output Layer</td><td>2.40112</td></tr><tr><td>Validation Accuracy</td><td>0.53521</td></tr><tr><td>Validation Accuracy Epoch</td><td>0.16901</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>0.37622</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>0.38965</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>0.38281</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>0.36865</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>0.36938</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>0.77637</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">Seed 55</strong>: <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/1o6a5hyu\" target=\"_blank\">https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/1o6a5hyu</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221020_054312-1o6a5hyu/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221020_054359-1t1zfj2l</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/1t1zfj2l\" target=\"_blank\">Seed 83</a></strong> to <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Training Loop Epoch: 0:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 0: 100%|██████████| 20/20 [00:02<00:00,  9.04it/s, Epoch: 0 | Training_accuracy: 0.5291 | Training Loss_avg: 2.4961 | Training Loss_step: 2.5339 | Learning Rate: 0.0000 | Grad: 3.7552]\n","Validation Loop Epoch: 0:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 0: 100%|██████████| 3/3 [00:00<00:00,  5.45it/s, Epoch: 0 | Validation Loss_avg: 2.4300 | Validation_accuracy_step: 0.591549277305603]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy rose from -inf to 0.5915 on epoch 0\n","Saved to model to .//seed-83.pt!\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 1:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 1: 100%|██████████| 20/20 [00:02<00:00,  8.96it/s, Epoch: 1 | Training_accuracy: 0.5118 | Training Loss_avg: 2.4626 | Training Loss_step: 2.4915 | Learning Rate: 0.0000 | Grad: 3.6824]\n","Validation Loop Epoch: 1:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 1: 100%|██████████| 3/3 [00:00<00:00,  5.14it/s, Epoch: 1 | Validation Loss_avg: 2.4597 | Validation_accuracy_step: 0.5492957830429077]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.5493 on epoch 1\n","Model did not improve on epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 2:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 2: 100%|██████████| 20/20 [00:02<00:00,  9.12it/s, Epoch: 2 | Training_accuracy: 0.5118 | Training Loss_avg: 2.4625 | Training Loss_step: 2.5304 | Learning Rate: 0.0000 | Grad: 3.8146]\n","Validation Loop Epoch: 2:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 2: 100%|██████████| 3/3 [00:00<00:00,  5.62it/s, Epoch: 2 | Validation Loss_avg: 2.4630 | Validation_accuracy_step: 0.4647887349128723]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.4648 on epoch 2\n","Model did not improve on epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 3:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 3: 100%|██████████| 20/20 [00:02<00:00,  9.10it/s, Epoch: 3 | Training_accuracy: 0.5260 | Training Loss_avg: 2.4502 | Training Loss_step: 2.5225 | Learning Rate: 0.0000 | Grad: 3.6042]\n","Validation Loop Epoch: 3:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 3: 100%|██████████| 3/3 [00:00<00:00,  5.32it/s, Epoch: 3 | Validation Loss_avg: 2.4970 | Validation_accuracy_step: 0.35211268067359924]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.3521 on epoch 3\n","Model did not improve on epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 4:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 4: 100%|██████████| 20/20 [00:02<00:00,  9.10it/s, Epoch: 4 | Training_accuracy: 0.5575 | Training Loss_avg: 2.4424 | Training Loss_step: 2.4204 | Learning Rate: 0.0000 | Grad: 3.8776]\n","Validation Loop Epoch: 4:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 4: 100%|██████████| 3/3 [00:00<00:00,  5.48it/s, Epoch: 4 | Validation Loss_avg: 2.5075 | Validation_accuracy_step: 0.2957746386528015]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.2958 on epoch 4\n","Model did not improve on epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 5:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 5: 100%|██████████| 20/20 [00:02<00:00,  9.07it/s, Epoch: 5 | Training_accuracy: 0.5024 | Training Loss_avg: 2.4492 | Training Loss_step: 2.6077 | Learning Rate: 0.0000 | Grad: 6.2166]\n","Validation Loop Epoch: 5:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 5: 100%|██████████| 3/3 [00:00<00:00,  5.55it/s, Epoch: 5 | Validation Loss_avg: 2.5231 | Validation_accuracy_step: 0.4225352108478546]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.4225 on epoch 5\n","Model did not improve on epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 6:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 6: 100%|██████████| 20/20 [00:02<00:00,  9.19it/s, Epoch: 6 | Training_accuracy: 0.5307 | Training Loss_avg: 2.4432 | Training Loss_step: 2.4450 | Learning Rate: 0.0000 | Grad: 3.7778]\n","Validation Loop Epoch: 6:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 6: 100%|██████████| 3/3 [00:00<00:00,  5.59it/s, Epoch: 6 | Validation Loss_avg: 2.5464 | Validation_accuracy_step: 0.2535211145877838]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.2535 on epoch 6\n","Model did not improve on epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 7:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 7: 100%|██████████| 20/20 [00:02<00:00,  9.21it/s, Epoch: 7 | Training_accuracy: 0.5402 | Training Loss_avg: 2.4304 | Training Loss_step: 2.5626 | Learning Rate: 0.0000 | Grad: 2.8352]\n","Validation Loop Epoch: 7:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 7: 100%|██████████| 3/3 [00:00<00:00,  5.37it/s, Epoch: 7 | Validation Loss_avg: 2.5717 | Validation_accuracy_step: 0.26760563254356384]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.2676 on epoch 7\n","Model did not improve on epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 8:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 8: 100%|██████████| 20/20 [00:02<00:00,  9.17it/s, Epoch: 8 | Training_accuracy: 0.5071 | Training Loss_avg: 2.4391 | Training Loss_step: 2.4538 | Learning Rate: 0.0000 | Grad: 3.1884]\n","Validation Loop Epoch: 8:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 8: 100%|██████████| 3/3 [00:00<00:00,  5.44it/s, Epoch: 8 | Validation Loss_avg: 2.6084 | Validation_accuracy_step: 0.1690140813589096]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.1690 on epoch 8\n","Model did not improve on epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 9:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 9: 100%|██████████| 20/20 [00:02<00:00,  9.11it/s, Epoch: 9 | Training_accuracy: 0.5323 | Training Loss_avg: 2.4289 | Training Loss_step: 2.4072 | Learning Rate: 0.0000 | Grad: 2.8566]\n","Validation Loop Epoch: 9:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 9: 100%|██████████| 3/3 [00:00<00:00,  5.52it/s, Epoch: 9 | Validation Loss_avg: 2.6305 | Validation_accuracy_step: 0.1690140813589096]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.1690 on epoch 9\n","Model did not improve on epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Loop Epoch: 10:   0%|          | 0/20 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Training Loop Epoch: 10: 100%|██████████| 20/20 [00:02<00:00,  9.16it/s, Epoch: 10 | Training_accuracy: 0.5323 | Training Loss_avg: 2.4193 | Training Loss_step: 2.4929 | Learning Rate: 0.0000 | Grad: 3.3781]\n","Validation Loop Epoch: 10:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Validation Loop Epoch: 10: 100%|██████████| 3/3 [00:00<00:00,  5.50it/s, Epoch: 10 | Validation Loss_avg: 2.6648 | Validation_accuracy_step: 0.1690140813589096]\n"]},{"output_type":"stream","name":"stdout","text":["Validation accuracy fell from 0.5915 to 0.1690 on epoch 10\n","Model did not improve on epoch 10\n","Stopping the model at epoch 10 since the model did not improve!\n"]},{"output_type":"stream","name":"stderr","text":["\rGetting Test Predictions:   0%|          | 0/3 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Getting Test Predictions: 100%|██████████| 3/3 [00:00<00:00,  5.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.591549277305603\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.650 MB of 0.650 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85978db29694ff0818e9c738f13c9d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>█▅▅▄▃▄▃▂▃▂▁</td></tr><tr><td>Overall Validation Loss Epoch</td><td>▁▂▂▃▃▄▄▅▆▇█</td></tr><tr><td>Training Accuracy Epoch</td><td>▄▂▂▄█▁▅▆▂▅▅</td></tr><tr><td>Training Accuracy Step</td><td>▂▆▅▄▁▂▃▃▂▃▃█▅▄▄▃▄▄▂▄▄▃▄▄▄▄▂▃▄▃▂▃▃▃▄▄▁▄▄▄</td></tr><tr><td>Training Loss Step Layer 10</td><td>▂█▁▄▃▂▆▃▂▁▁▃▄▄▄▃▄▆▃▄▃▃▅▃▄▁▆▄▄▃▆▄▂▅▃▅▃▁▂▂</td></tr><tr><td>Training Loss Step Layer 11</td><td>▇▅▇▆▅█▅▄▄▅▅▄▄▆▅▄▂▃▅▂▁▂▂▆▆▇▄▃▃▄▂▅▄▄▂▄▄▄▂▅</td></tr><tr><td>Training Loss Step Layer 12</td><td>█▇▅▄▄▁▄▆▇▆▄▇▄█▆▅▆▄▅▄▂▅▅▅▄▄▇▄▄▃▇▃▇▄▃▄▆▆▅▆</td></tr><tr><td>Training Loss Step Layer 8</td><td>▅▅▇█▄▆▄▅▁▃▃▃▅▅▇▄▅█▆▇▃▅▆▅▇▁▅▆▄▃▆▅▅▄▆▆▃▃▄▇</td></tr><tr><td>Training Loss Step Layer 9</td><td>▆█▆▅▃▇▄▅▆▃▆▂▆▅▄▃▄▆▇▄▄▅▁▄▄▆▅▅▃▆▃▄▂▇▄▄▅▅▅▆</td></tr><tr><td>Training Loss Step Output Layer</td><td>▇▇█▇▃▅▆▇▅▃▄▂▅▇█▄▄▇▇▆▁▅▃▅▅▅▆▅▃▂▆▄▅▅▄▃▅▃▃▇</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Accuracy Epoch</td><td>█▇▆▄▃▅▂▃▁▁▁</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>▁▄▃▄▄▄▅▅▇▇█</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>▁▂▂▃▃▄▅▆▇▇█</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>▁▁▁▃▃▄▄▅▆▇█</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>▁▄▃▅▄▅▅▆▇▇█</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>▁▂▂▅▆▃▄▅▅▆█</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>▁▂▂▃▃▄▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Overall Training Loss Epoch</td><td>2.41931</td></tr><tr><td>Overall Validation Loss Epoch</td><td>2.66478</td></tr><tr><td>Training Accuracy Epoch</td><td>0.53228</td></tr><tr><td>Training Accuracy Step</td><td>0.53228</td></tr><tr><td>Training Loss Step Layer 10</td><td>0.34186</td></tr><tr><td>Training Loss Step Layer 11</td><td>0.35699</td></tr><tr><td>Training Loss Step Layer 12</td><td>0.35635</td></tr><tr><td>Training Loss Step Layer 8</td><td>0.3688</td></tr><tr><td>Training Loss Step Layer 9</td><td>0.36717</td></tr><tr><td>Training Loss Step Output Layer</td><td>2.49292</td></tr><tr><td>Validation Accuracy</td><td>0.59155</td></tr><tr><td>Validation Accuracy Epoch</td><td>0.16901</td></tr><tr><td>Validation Loss Epoch Layer 10</td><td>0.36401</td></tr><tr><td>Validation Loss Epoch Layer 11</td><td>0.39209</td></tr><tr><td>Validation Loss Epoch Layer 12</td><td>0.41187</td></tr><tr><td>Validation Loss Epoch Layer 8</td><td>0.35938</td></tr><tr><td>Validation Loss Epoch Layer 9</td><td>0.35156</td></tr><tr><td>Validation Loss Epoch Output Layer</td><td>0.78662</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">Seed 83</strong>: <a href=\"https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/1t1zfj2l\" target=\"_blank\">https://wandb.ai/goggins/Auxillary%20BERT%20Research/runs/1t1zfj2l</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221020_054359-1t1zfj2l/logs</code>"]},"metadata":{}}],"source":["train_loop(train, val, test, data_collator, CFG, device)"]},{"cell_type":"code","source":[],"metadata":{"id":"WRuuJZIAYz5U","executionInfo":{"status":"ok","timestamp":1666244687103,"user_tz":420,"elapsed":7,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":29,"metadata":{"id":"HrfPAilwoOH5","executionInfo":{"status":"ok","timestamp":1666244687103,"user_tz":420,"elapsed":6,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{"id":"XcQCJvBgDGMb","executionInfo":{"status":"ok","timestamp":1666244687103,"user_tz":420,"elapsed":6,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{"id":"548_d2-Xyd_-","executionInfo":{"status":"ok","timestamp":1666244687104,"user_tz":420,"elapsed":7,"user":{"displayName":"Leo Xu","userId":"06749631020377743695"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1iZlCeL0v9o6fEt1wstRlgV371pOB2kgi","timestamp":1664493650741}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"11f1b0e39c524396b111e82f986b364a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b646c9638d24367837d1e279f3b99f0","IPY_MODEL_f0f9c190788144eca4920aae3acce3d2","IPY_MODEL_a248852be4e0465ab4e717481e1a6127"],"layout":"IPY_MODEL_050e4048b34645c7b9e0a2c7dc85dcf6"}},"7b646c9638d24367837d1e279f3b99f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41292a553db64ee3b876cf9a9396ae8b","placeholder":"​","style":"IPY_MODEL_c199156a527141fa82d6370f8bf4ed32","value":"Downloading: 100%"}},"f0f9c190788144eca4920aae3acce3d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_073724166d3e4d50ad94c9532d77e462","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce93b716ab6242c39e34c87fa8a4d764","value":28}},"a248852be4e0465ab4e717481e1a6127":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bebb3153c7f049ebaa9a01f8a845b326","placeholder":"​","style":"IPY_MODEL_f4efbcde03cf420487aa95a0b6243a65","value":" 28.0/28.0 [00:00&lt;00:00, 791B/s]"}},"050e4048b34645c7b9e0a2c7dc85dcf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41292a553db64ee3b876cf9a9396ae8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c199156a527141fa82d6370f8bf4ed32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"073724166d3e4d50ad94c9532d77e462":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce93b716ab6242c39e34c87fa8a4d764":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bebb3153c7f049ebaa9a01f8a845b326":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4efbcde03cf420487aa95a0b6243a65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"398a705abf80414b878508c90f0d012e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9199eb0ac7144d59b0dbba3abc2f287a","IPY_MODEL_bf3d90313b094654b85a725596d232f4","IPY_MODEL_96426062d5904326b0778bef8dce50b8"],"layout":"IPY_MODEL_2dd4804cb41c4548bd8e5ea67156181a"}},"9199eb0ac7144d59b0dbba3abc2f287a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4385b076b174108adc0700d1d5014f3","placeholder":"​","style":"IPY_MODEL_78f4165fe3e24a7eba5e408e10351edd","value":"Downloading: 100%"}},"bf3d90313b094654b85a725596d232f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b7b104cc4c44fdb92e5236a418711de","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f080a254aaba48b78502e8d6ef234858","value":570}},"96426062d5904326b0778bef8dce50b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c53725da394a4c508a719602075629cf","placeholder":"​","style":"IPY_MODEL_5b00fc367f35490e8af763e13441e594","value":" 570/570 [00:00&lt;00:00, 22.3kB/s]"}},"2dd4804cb41c4548bd8e5ea67156181a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4385b076b174108adc0700d1d5014f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78f4165fe3e24a7eba5e408e10351edd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b7b104cc4c44fdb92e5236a418711de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f080a254aaba48b78502e8d6ef234858":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c53725da394a4c508a719602075629cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b00fc367f35490e8af763e13441e594":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5941a8d087546308792b5da4db052ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9d99dbecc89493aab658b98c0b1643a","IPY_MODEL_5cb94fed351d434596320cb2f7aabfff","IPY_MODEL_41512e8aaed24f808331af0e70693543"],"layout":"IPY_MODEL_1e98828eb00745e0a53a32cb04d7bfe4"}},"e9d99dbecc89493aab658b98c0b1643a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7edc7f7c98fc4f309fb422aff2d270c0","placeholder":"​","style":"IPY_MODEL_84ee15ea3e584fe486ec1017894b73d0","value":"Downloading: 100%"}},"5cb94fed351d434596320cb2f7aabfff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe613a0d99c14387a5ce65fc5dda808c","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1fa40ffd4b2b402cba7348f4169f8620","value":231508}},"41512e8aaed24f808331af0e70693543":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c30e0993bce451e988d221ee133ac2a","placeholder":"​","style":"IPY_MODEL_6459b144543943888610be4d17a16cd9","value":" 232k/232k [00:00&lt;00:00, 1.68MB/s]"}},"1e98828eb00745e0a53a32cb04d7bfe4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7edc7f7c98fc4f309fb422aff2d270c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84ee15ea3e584fe486ec1017894b73d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe613a0d99c14387a5ce65fc5dda808c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fa40ffd4b2b402cba7348f4169f8620":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c30e0993bce451e988d221ee133ac2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6459b144543943888610be4d17a16cd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c4d8c7ee6e84397a41bc6be08ecbf68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36bfbbb1b2e24b83a7608230a485e3f8","IPY_MODEL_923575645a5c404690e86ae2375d3e32","IPY_MODEL_68a84fb8977a4335aae056b8c9fb5372"],"layout":"IPY_MODEL_bc0a1748b6c64207bbbc4461a8cff7a5"}},"36bfbbb1b2e24b83a7608230a485e3f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed6e73738e4b4639b4549fe28bf1a4cb","placeholder":"​","style":"IPY_MODEL_f5bdf1d15acd4334817e434d0e4fdc97","value":"Downloading: 100%"}},"923575645a5c404690e86ae2375d3e32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7270f970347a4ccc92ee949236295b0e","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b46a3a9c955498b8d178ba3a34a8c26","value":466062}},"68a84fb8977a4335aae056b8c9fb5372":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71ac9b98da224b1182d8cf2c351f6d8e","placeholder":"​","style":"IPY_MODEL_574b6714fab94763be99107455a63ab5","value":" 466k/466k [00:00&lt;00:00, 1.78MB/s]"}},"bc0a1748b6c64207bbbc4461a8cff7a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed6e73738e4b4639b4549fe28bf1a4cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5bdf1d15acd4334817e434d0e4fdc97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7270f970347a4ccc92ee949236295b0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b46a3a9c955498b8d178ba3a34a8c26":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71ac9b98da224b1182d8cf2c351f6d8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"574b6714fab94763be99107455a63ab5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdb148737c9344ed86394b0ea629aecf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2f7b7b54adf4da0aefdcc9af0ef00c7","IPY_MODEL_0485e767f9a144598d94d98d70187a66","IPY_MODEL_2aba8695fafc4455b23b70717d12e733"],"layout":"IPY_MODEL_6c939388d5604e788f22e2f43d7d9be0"}},"a2f7b7b54adf4da0aefdcc9af0ef00c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_609a69ba23ea43efaca0c8f331d4ada1","placeholder":"​","style":"IPY_MODEL_5f39f6f021ac49e7b521fa5b79decd92","value":"Downloading builder script: 100%"}},"0485e767f9a144598d94d98d70187a66":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5200ac118f494ff1aab4ad9eb1a0d05c","max":28751,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc53030cb7eb4464b023b83d1d505d81","value":28751}},"2aba8695fafc4455b23b70717d12e733":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f36125d55ef946dc971c1eec38948a3a","placeholder":"​","style":"IPY_MODEL_88d60daf5f254a0fa13b0f0b0e9dd1be","value":" 28.8k/28.8k [00:00&lt;00:00, 985kB/s]"}},"6c939388d5604e788f22e2f43d7d9be0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"609a69ba23ea43efaca0c8f331d4ada1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f39f6f021ac49e7b521fa5b79decd92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5200ac118f494ff1aab4ad9eb1a0d05c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc53030cb7eb4464b023b83d1d505d81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f36125d55ef946dc971c1eec38948a3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88d60daf5f254a0fa13b0f0b0e9dd1be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3149f0f38af4434e952e66092e6c00da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aec1cf7bb1de46b59c442af4a34c7840","IPY_MODEL_872fa7b95469496abacf1f9cd91012db","IPY_MODEL_2a6265c85e5e48f4a18b5e3babbc0f85"],"layout":"IPY_MODEL_5247899cc8bc46c792ca18ae392b367d"}},"aec1cf7bb1de46b59c442af4a34c7840":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b840c7cfab042c58c4bc7dea2f9bd07","placeholder":"​","style":"IPY_MODEL_e01fac78d21e4432b3f33f0ab596360e","value":"Downloading metadata: 100%"}},"872fa7b95469496abacf1f9cd91012db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1eca2b205fdf49a98d893182dca0b2cd","max":28682,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5006200a961a45e9875eb5a12fc64a25","value":28682}},"2a6265c85e5e48f4a18b5e3babbc0f85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_725570f3eade41c9ba62cfd76fe7411e","placeholder":"​","style":"IPY_MODEL_984658afb74a42af8a339490e1999acd","value":" 28.7k/28.7k [00:00&lt;00:00, 1.01MB/s]"}},"5247899cc8bc46c792ca18ae392b367d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b840c7cfab042c58c4bc7dea2f9bd07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e01fac78d21e4432b3f33f0ab596360e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1eca2b205fdf49a98d893182dca0b2cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5006200a961a45e9875eb5a12fc64a25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"725570f3eade41c9ba62cfd76fe7411e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"984658afb74a42af8a339490e1999acd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e50e8a2cb59b4a1aae94e9ca685ed2d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccca90574ac14e758d1a2c6c3121ab33","IPY_MODEL_1bca0faf847c49f6a5cfd9049f862620","IPY_MODEL_97a2c5e36c594774805b1028dca5325f"],"layout":"IPY_MODEL_9fd6223accd640bf956f862c30660657"}},"ccca90574ac14e758d1a2c6c3121ab33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5f121bfcf5f40b8beb5d12591912e37","placeholder":"​","style":"IPY_MODEL_68b9e533a5a4488c93ea1162f797de5a","value":"Downloading readme: 100%"}},"1bca0faf847c49f6a5cfd9049f862620":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d6b130c30a249e99af610d5cfe5a48d","max":21988,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84af968f4f874117b91426cac42814fc","value":21988}},"97a2c5e36c594774805b1028dca5325f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3edc6b9cfc874feab738709cedd9ec7c","placeholder":"​","style":"IPY_MODEL_82e64da19b17441db6ce7423a92d0342","value":" 22.0k/22.0k [00:00&lt;00:00, 730kB/s]"}},"9fd6223accd640bf956f862c30660657":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5f121bfcf5f40b8beb5d12591912e37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68b9e533a5a4488c93ea1162f797de5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d6b130c30a249e99af610d5cfe5a48d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84af968f4f874117b91426cac42814fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3edc6b9cfc874feab738709cedd9ec7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82e64da19b17441db6ce7423a92d0342":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"420f934c525d48ef9847311f76de971e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2b8b93ec88d4b4fadfb0bbd47e040d2","IPY_MODEL_98e0eeaa1f4e45bc8e05aea33f0f3756","IPY_MODEL_17ed059ba1ba431e8e53bfa9308021bb"],"layout":"IPY_MODEL_08b81912ed3e444bb086746071902a24"}},"c2b8b93ec88d4b4fadfb0bbd47e040d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_281ee8a4628c4c20b677c0f50f040779","placeholder":"​","style":"IPY_MODEL_71a4922ab9434b21a3a99f774e33d4f0","value":"Downloading data: 100%"}},"98e0eeaa1f4e45bc8e05aea33f0f3756":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_919752d73cd546e8b9a66639b95028e5","max":28999,"min":0,"orientation":"horizontal","style":"IPY_MODEL_210c50fc752d47df815dca424e861fee","value":28999}},"17ed059ba1ba431e8e53bfa9308021bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c386996b548841c7bbe6f0d8b356b71c","placeholder":"​","style":"IPY_MODEL_0dfe7dcc9c2a4988b836584ebe319a0d","value":" 29.0k/29.0k [00:00&lt;00:00, 497kB/s]"}},"08b81912ed3e444bb086746071902a24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"281ee8a4628c4c20b677c0f50f040779":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71a4922ab9434b21a3a99f774e33d4f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"919752d73cd546e8b9a66639b95028e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"210c50fc752d47df815dca424e861fee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c386996b548841c7bbe6f0d8b356b71c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dfe7dcc9c2a4988b836584ebe319a0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"682afebb7e204bd1b5cc7010fc1c9304":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64ff9775c3c44df99f495bdc12877748","IPY_MODEL_cc6c58e6beca46618bd5e3a799ea13bf","IPY_MODEL_1cf09787aaf54753b0ed996a482c0924"],"layout":"IPY_MODEL_6ce786edf6a8498a8ffe9c92a8657cac"}},"64ff9775c3c44df99f495bdc12877748":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7e07c44cfa74da69391e0a4cadfda79","placeholder":"​","style":"IPY_MODEL_3b4adf1bcb3a45af9d85920ddc3241f2","value":"Generating train split:   0%"}},"cc6c58e6beca46618bd5e3a799ea13bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_be50f266fa8c40f7af6e3d58f4de7330","max":635,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26fdf1f28cbb4c9aa434320044373836","value":635}},"1cf09787aaf54753b0ed996a482c0924":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78a1bca8c95c4d5dac8882c8580cf5fd","placeholder":"​","style":"IPY_MODEL_a77cb6b371ae401381bd3cc7604e096e","value":" 0/635 [00:00&lt;?, ? examples/s]"}},"6ce786edf6a8498a8ffe9c92a8657cac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d7e07c44cfa74da69391e0a4cadfda79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b4adf1bcb3a45af9d85920ddc3241f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be50f266fa8c40f7af6e3d58f4de7330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26fdf1f28cbb4c9aa434320044373836":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78a1bca8c95c4d5dac8882c8580cf5fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a77cb6b371ae401381bd3cc7604e096e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8948084a9abe4673ba3b95d28f2a6b7c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e42bb9c2aac4ec4950677ffd7661799","IPY_MODEL_f3690b2815174b908f1a9be1a7dbfe26","IPY_MODEL_a5eed7781c774e44afa5500209728ad3"],"layout":"IPY_MODEL_0c19ff9fa08f44a3846a6cd7a1feb3a3"}},"8e42bb9c2aac4ec4950677ffd7661799":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd6aa21a64174b92bed3747de2c63a85","placeholder":"​","style":"IPY_MODEL_994e2b4d1a9b436aab6162015d0af8c4","value":"Generating validation split:   0%"}},"f3690b2815174b908f1a9be1a7dbfe26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bbe9000d9454406a5bb840675d5e31e","max":71,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8893a1c01cd54910b9c9f826cf2eeba8","value":71}},"a5eed7781c774e44afa5500209728ad3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f670dacee284026902ae99508b65a83","placeholder":"​","style":"IPY_MODEL_1e7b28a3c3e841c9b4437cc478643780","value":" 0/71 [00:00&lt;?, ? examples/s]"}},"0c19ff9fa08f44a3846a6cd7a1feb3a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"dd6aa21a64174b92bed3747de2c63a85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"994e2b4d1a9b436aab6162015d0af8c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bbe9000d9454406a5bb840675d5e31e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8893a1c01cd54910b9c9f826cf2eeba8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f670dacee284026902ae99508b65a83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e7b28a3c3e841c9b4437cc478643780":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8eb2887355f24b70bf26d39ef7628a16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfe42ba11dce4772ab855897b3bb76b2","IPY_MODEL_8c569b2073fa4562aa506f2be184a1f1","IPY_MODEL_5c7b6624964e4a4eaa480eee80ca5ddb"],"layout":"IPY_MODEL_a0ee4aa123e345f49096b4d9e1e80843"}},"cfe42ba11dce4772ab855897b3bb76b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05a6f4b180434f6189256f5993f4477d","placeholder":"​","style":"IPY_MODEL_f90dda0ac00741c0a533b60859056beb","value":"Generating test split:   0%"}},"8c569b2073fa4562aa506f2be184a1f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee405f3411424215a7b6330d6d582b63","max":146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4913b8dbf3334bef8a1fd35966154038","value":146}},"5c7b6624964e4a4eaa480eee80ca5ddb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_376b10741dd849259cc3739b6b5f6438","placeholder":"​","style":"IPY_MODEL_011d1a16df6a4272aafbf58667d6c014","value":" 0/146 [00:00&lt;?, ? examples/s]"}},"a0ee4aa123e345f49096b4d9e1e80843":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"05a6f4b180434f6189256f5993f4477d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90dda0ac00741c0a533b60859056beb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee405f3411424215a7b6330d6d582b63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4913b8dbf3334bef8a1fd35966154038":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"376b10741dd849259cc3739b6b5f6438":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"011d1a16df6a4272aafbf58667d6c014":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed8f312f56e140d7ba1b02f5828cb4c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b6292a8b6fb46d8a013a20464649b5e","IPY_MODEL_d2adb16e0b7f4d3c92d146dbd6f864f8","IPY_MODEL_2bad6d65585845e38b76ef4ce4a914cd"],"layout":"IPY_MODEL_c97fe2b52b374519a3bebe57fb64f9f2"}},"7b6292a8b6fb46d8a013a20464649b5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7260598dfb284378a05e7a78728fe0ed","placeholder":"​","style":"IPY_MODEL_a000eea6a0e843fe96df1c73a47eee6b","value":"100%"}},"d2adb16e0b7f4d3c92d146dbd6f864f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0f78b01c04d42ac8652edd6ca9c9d95","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc95751c114346b5a4c1219bf10372a4","value":3}},"2bad6d65585845e38b76ef4ce4a914cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec496839eb884891b0c3cb9f0d966ce3","placeholder":"​","style":"IPY_MODEL_f10e2101b1444431bd56d490a4b5ba9c","value":" 3/3 [00:00&lt;00:00, 71.00it/s]"}},"c97fe2b52b374519a3bebe57fb64f9f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7260598dfb284378a05e7a78728fe0ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a000eea6a0e843fe96df1c73a47eee6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0f78b01c04d42ac8652edd6ca9c9d95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc95751c114346b5a4c1219bf10372a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec496839eb884891b0c3cb9f0d966ce3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f10e2101b1444431bd56d490a4b5ba9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59cc7d68291442e3883b264c57f235d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c95104a01c1b42f78477ab50f50f7bf4","IPY_MODEL_7692c19bde4c474ca734ca8ac52f946a","IPY_MODEL_8d99985ad6c7452fa2e22f97fbe9a9e2"],"layout":"IPY_MODEL_743a4ebd1a4949dea03e7004179eba8b"}},"c95104a01c1b42f78477ab50f50f7bf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28ae51470bb74d2f9d48e84e68c6a80b","placeholder":"​","style":"IPY_MODEL_ebd2c603d2374d97b8da02b4f990322d","value":"#0:   0%"}},"7692c19bde4c474ca734ca8ac52f946a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd803936ba9b48fa89431014128b895b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c6c2021d0a542b499af67607a907882","value":0}},"8d99985ad6c7452fa2e22f97fbe9a9e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_089c3034152c4496bcd69f63af329b93","placeholder":"​","style":"IPY_MODEL_d4d9522cf1f74499b8affb6549be062c","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"743a4ebd1a4949dea03e7004179eba8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ae51470bb74d2f9d48e84e68c6a80b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebd2c603d2374d97b8da02b4f990322d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd803936ba9b48fa89431014128b895b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c6c2021d0a542b499af67607a907882":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"089c3034152c4496bcd69f63af329b93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4d9522cf1f74499b8affb6549be062c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"563bd3b228144cdf9f9cacfa1ef5bda3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74f27cbe76bf4fd8b88348255f92dbf1","IPY_MODEL_4e6fb87691354ebabb207abe5bf79c70","IPY_MODEL_97c774933fbc41e38676eb2d395f5527"],"layout":"IPY_MODEL_a691648f1a254f298da9cc52217fb85e"}},"74f27cbe76bf4fd8b88348255f92dbf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a269031086d74039919a37d352108f6c","placeholder":"​","style":"IPY_MODEL_6db88c663ba94e78aecc60ebfe780148","value":"#1:   0%"}},"4e6fb87691354ebabb207abe5bf79c70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d6507851ea464da1e48eec8ae77b63","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_47390c882f664c108785980121286eb3","value":0}},"97c774933fbc41e38676eb2d395f5527":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec96832905154cfabe143c2780c0b2d1","placeholder":"​","style":"IPY_MODEL_4e7aab0f51954a02b48ae16adff509af","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"a691648f1a254f298da9cc52217fb85e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a269031086d74039919a37d352108f6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6db88c663ba94e78aecc60ebfe780148":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52d6507851ea464da1e48eec8ae77b63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47390c882f664c108785980121286eb3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec96832905154cfabe143c2780c0b2d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e7aab0f51954a02b48ae16adff509af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2894677b67a4eeaa13d3aea2f1370d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51e3c024514a48fb9d2ecd7bf28117af","IPY_MODEL_921a7e3f17e648b79e242e9b24551918","IPY_MODEL_9f57996c2f964a47b85acd4da9effc34"],"layout":"IPY_MODEL_5a395d38e16c403eb9d94b3a25ab34d2"}},"51e3c024514a48fb9d2ecd7bf28117af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dec9e750291f4b7b94448c2fd2f49181","placeholder":"​","style":"IPY_MODEL_7be5ba863c5a4befa3f8878636462cb5","value":"#2:   0%"}},"921a7e3f17e648b79e242e9b24551918":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_48762ef76ed241ddb1fb1aaadb3f701c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_227821b436a64a418071bd11f60744c9","value":0}},"9f57996c2f964a47b85acd4da9effc34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d408e23b7c0d4db4ba850317cd2a9d0e","placeholder":"​","style":"IPY_MODEL_f8fff800dd404b929036b676c31524b2","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"5a395d38e16c403eb9d94b3a25ab34d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dec9e750291f4b7b94448c2fd2f49181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7be5ba863c5a4befa3f8878636462cb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48762ef76ed241ddb1fb1aaadb3f701c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227821b436a64a418071bd11f60744c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d408e23b7c0d4db4ba850317cd2a9d0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8fff800dd404b929036b676c31524b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c57122d834594856b0ee3c384b925406":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e859fb6c7e2a4c23bbfc82265baa3af2","IPY_MODEL_beb822d9b96f4f16ad843289133b437b","IPY_MODEL_37353a6131f5423c8777cd3c97844a96"],"layout":"IPY_MODEL_9b563da356624bbbbe26ede8d754ff12"}},"e859fb6c7e2a4c23bbfc82265baa3af2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bfdc0611e8f4b398650aefe8efe3e16","placeholder":"​","style":"IPY_MODEL_7dfcdd278d0d49dea86349b762f91b54","value":"#3:   0%"}},"beb822d9b96f4f16ad843289133b437b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f54821a5e234489abadd7db4d1bc2c19","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_477a2422198743f8962379cc8607d639","value":0}},"37353a6131f5423c8777cd3c97844a96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a3f2ecf15543d4aeee27a23fecff11","placeholder":"​","style":"IPY_MODEL_707475ee171c4132966f11b0e3455f3f","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"9b563da356624bbbbe26ede8d754ff12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bfdc0611e8f4b398650aefe8efe3e16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dfcdd278d0d49dea86349b762f91b54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f54821a5e234489abadd7db4d1bc2c19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"477a2422198743f8962379cc8607d639":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a3f2ecf15543d4aeee27a23fecff11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"707475ee171c4132966f11b0e3455f3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e558ced6ebcc46c3937d0413f78304dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1a2c4113e034888b7c73ace0af4811a","IPY_MODEL_1a73e00d24074d62bd2e49292d72404e","IPY_MODEL_a2524333ece8448a86d0d213df1f05dc"],"layout":"IPY_MODEL_1d682577364746dcb3e7730372f3da73"}},"d1a2c4113e034888b7c73ace0af4811a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54ab1fa881bb47a085b339b00eabe1ad","placeholder":"​","style":"IPY_MODEL_95f88395ce194b519e3911e08fbb9d17","value":"#4:   0%"}},"1a73e00d24074d62bd2e49292d72404e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_a122125680c74398b682cc76a4687773","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_989dd129bc8145ceaf1750cd9373b964","value":0}},"a2524333ece8448a86d0d213df1f05dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee8046d32cc949aea74a6089f9fa0e4a","placeholder":"​","style":"IPY_MODEL_08b20c84d5a54f0b91b9072bc7e85f77","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"1d682577364746dcb3e7730372f3da73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54ab1fa881bb47a085b339b00eabe1ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95f88395ce194b519e3911e08fbb9d17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a122125680c74398b682cc76a4687773":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"989dd129bc8145ceaf1750cd9373b964":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee8046d32cc949aea74a6089f9fa0e4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b20c84d5a54f0b91b9072bc7e85f77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecd47314ec2441d491641609e46996ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf7ba6428cce4160ad1587e81b86282f","IPY_MODEL_5bef26a4519648f1b53f79217b175594","IPY_MODEL_e365e31d7a0746139231ff328b1983c4"],"layout":"IPY_MODEL_75878a03073a4ee4b5bce7b31a83a28a"}},"bf7ba6428cce4160ad1587e81b86282f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0340f423ed34da08c62c54a1d8fca0e","placeholder":"​","style":"IPY_MODEL_1f4b80fc0dee4046977d6dc034280202","value":"#5:   0%"}},"5bef26a4519648f1b53f79217b175594":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_45db3374b2a14ddb96c540c3bf5f62d6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff7addbe02594389abc6a626d7c233f7","value":0}},"e365e31d7a0746139231ff328b1983c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_443e1e7538d44164863907f602e7759c","placeholder":"​","style":"IPY_MODEL_68798f7d8be140609c730655e5eb5686","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"75878a03073a4ee4b5bce7b31a83a28a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0340f423ed34da08c62c54a1d8fca0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f4b80fc0dee4046977d6dc034280202":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45db3374b2a14ddb96c540c3bf5f62d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff7addbe02594389abc6a626d7c233f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"443e1e7538d44164863907f602e7759c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68798f7d8be140609c730655e5eb5686":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53ce5c73b7f44ea3a99985bb45260905":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41aba5ee54824a91b82a816ae9f6e155","IPY_MODEL_6b8d1d481b2349d5891981a04bb438c4","IPY_MODEL_e08e8789b74b4da1ab5b526f6ab684d2"],"layout":"IPY_MODEL_1cad84b31db64218b533d0f2dec6d460"}},"41aba5ee54824a91b82a816ae9f6e155":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c026381b9d04b17836f1c55c35cd619","placeholder":"​","style":"IPY_MODEL_b65df35d661d4447b3cdb19a8a684140","value":"#6:   0%"}},"6b8d1d481b2349d5891981a04bb438c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_2729fb4ab9e94d03aaa11d5a7ae6b708","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bddc94aa3ec54fbfbd9b2bccfd363321","value":0}},"e08e8789b74b4da1ab5b526f6ab684d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2176c70770ea4a2b9c22c54193521727","placeholder":"​","style":"IPY_MODEL_ec26de6a90374a32b833fa6292cf78db","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"1cad84b31db64218b533d0f2dec6d460":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c026381b9d04b17836f1c55c35cd619":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b65df35d661d4447b3cdb19a8a684140":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2729fb4ab9e94d03aaa11d5a7ae6b708":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bddc94aa3ec54fbfbd9b2bccfd363321":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2176c70770ea4a2b9c22c54193521727":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec26de6a90374a32b833fa6292cf78db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67cb95ffe32d48b6b594b4a652e42857":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d40c51dcc7b04cba8a0131f19782eebc","IPY_MODEL_adaa54ef20bc47cd8188f76fa1b322a5","IPY_MODEL_fe2f299c951445edbe5221fb51cd57ba"],"layout":"IPY_MODEL_4d7266ffa14e45e0959d8d11ebaac5b7"}},"d40c51dcc7b04cba8a0131f19782eebc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68d19d5cf0d64aa19f471615ddcc3ffd","placeholder":"​","style":"IPY_MODEL_cbcf3f63f23246b48fed256e58bfa393","value":"#7:   0%"}},"adaa54ef20bc47cd8188f76fa1b322a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_547e46d05af7402ca06980169b40321a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a420116441f344f18a5ac8f3ad497ce3","value":0}},"fe2f299c951445edbe5221fb51cd57ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9110e91e4e7c4036a8d9c88d04dcc81c","placeholder":"​","style":"IPY_MODEL_1664581f0ae24ed18d53041b4831188f","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"4d7266ffa14e45e0959d8d11ebaac5b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68d19d5cf0d64aa19f471615ddcc3ffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbcf3f63f23246b48fed256e58bfa393":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"547e46d05af7402ca06980169b40321a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a420116441f344f18a5ac8f3ad497ce3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9110e91e4e7c4036a8d9c88d04dcc81c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1664581f0ae24ed18d53041b4831188f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fd11182a4bf4053a54e77965db7bc9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_344f90a8e6084a20800e4c561ec90066","IPY_MODEL_a759a23c165249ee8782cb331fca6c5b","IPY_MODEL_2696194cb66c463caa1bde8a213a5440"],"layout":"IPY_MODEL_31d0cc8bbb4a45f898aa4bf9c47335f5"}},"344f90a8e6084a20800e4c561ec90066":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24e3444ea0dc4ac4876ce7f4ff011f27","placeholder":"​","style":"IPY_MODEL_5b5bce0ac27340b98021a297a7ba38cc","value":"#0:   0%"}},"a759a23c165249ee8782cb331fca6c5b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_b76a18664b30408b8a15e87942a8e452","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e0f3e0920db4497a1879ec8c659432c","value":0}},"2696194cb66c463caa1bde8a213a5440":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af075d6f8abc49b7ae336de7a0178796","placeholder":"​","style":"IPY_MODEL_1f1f266fef2740babaa1d82e0df6cf00","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"31d0cc8bbb4a45f898aa4bf9c47335f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24e3444ea0dc4ac4876ce7f4ff011f27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b5bce0ac27340b98021a297a7ba38cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b76a18664b30408b8a15e87942a8e452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e0f3e0920db4497a1879ec8c659432c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af075d6f8abc49b7ae336de7a0178796":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f1f266fef2740babaa1d82e0df6cf00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34449fb324244c89a3014c274845117d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4596ee37c68c4415a83d5b4143a96fe6","IPY_MODEL_5bd071267fb9493db2a774254c16b20f","IPY_MODEL_34f743ef53144fb797383c7377d03d88"],"layout":"IPY_MODEL_5f6e1a969ee74d49913b7ee0dc3e4ba6"}},"4596ee37c68c4415a83d5b4143a96fe6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d4e1919bf014f5aa4f197254f6388f0","placeholder":"​","style":"IPY_MODEL_8243e5c2f77a401a8e38e0dd1b272037","value":"#1:   0%"}},"5bd071267fb9493db2a774254c16b20f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_877de1458c534b138d8311dffef32d27","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6f9da8c7b114304825736f08d95ef83","value":0}},"34f743ef53144fb797383c7377d03d88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6173d2290e403bb8608ff436b9ce10","placeholder":"​","style":"IPY_MODEL_32491a2e2b4246a08af989e0a9a79878","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"5f6e1a969ee74d49913b7ee0dc3e4ba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d4e1919bf014f5aa4f197254f6388f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8243e5c2f77a401a8e38e0dd1b272037":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"877de1458c534b138d8311dffef32d27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6f9da8c7b114304825736f08d95ef83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c6173d2290e403bb8608ff436b9ce10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32491a2e2b4246a08af989e0a9a79878":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b413751041c8478b8f150bbd26fafe56":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2abb1e4592d244c08db6ae1abd96f329","IPY_MODEL_b9f841f3bc4b4899a543fe01d79a1fd2","IPY_MODEL_06355a4c25e5406a85ca317bbe0e94f9"],"layout":"IPY_MODEL_1a5045094cd14de0ab26fc14c10927d7"}},"2abb1e4592d244c08db6ae1abd96f329":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc01847c749e49328eb85250bb3dd953","placeholder":"​","style":"IPY_MODEL_546c7c6c74b644a19b71e503db68493f","value":"#2:   0%"}},"b9f841f3bc4b4899a543fe01d79a1fd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_222e5ebc15264d99916e48bb24b9097d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06de1f64e28648c7809165e86e248fa7","value":0}},"06355a4c25e5406a85ca317bbe0e94f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7f41d5295084ee9b1657b0afd5d4654","placeholder":"​","style":"IPY_MODEL_8f666b18f86e4935a6bb0cc3121c6f24","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"1a5045094cd14de0ab26fc14c10927d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc01847c749e49328eb85250bb3dd953":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"546c7c6c74b644a19b71e503db68493f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"222e5ebc15264d99916e48bb24b9097d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06de1f64e28648c7809165e86e248fa7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7f41d5295084ee9b1657b0afd5d4654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f666b18f86e4935a6bb0cc3121c6f24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aa08cdcde784e48818988873eccb937":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5680d790c7834280a9c87a4e1c73ddd7","IPY_MODEL_f7bbc4fe5564454bb5125ca946c77d3a","IPY_MODEL_033c7c06e88540069e8dc3ae1a40a295"],"layout":"IPY_MODEL_602b45da191743b8a40d91a38df3e469"}},"5680d790c7834280a9c87a4e1c73ddd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6e7df33081141c5aa921e64ca636356","placeholder":"​","style":"IPY_MODEL_78c1841e700a4776b2db54fddfc8eae0","value":"#3:   0%"}},"f7bbc4fe5564454bb5125ca946c77d3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_20ab4a0f2c224ef0b3ea068fbf4ff3ad","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3948917894a449dd93627ccc84f622fa","value":0}},"033c7c06e88540069e8dc3ae1a40a295":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22f7df56de06429aade3aa3d93f7eb43","placeholder":"​","style":"IPY_MODEL_b2058b03f0644fa9b0ebee648d83dba5","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"602b45da191743b8a40d91a38df3e469":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6e7df33081141c5aa921e64ca636356":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78c1841e700a4776b2db54fddfc8eae0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20ab4a0f2c224ef0b3ea068fbf4ff3ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3948917894a449dd93627ccc84f622fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22f7df56de06429aade3aa3d93f7eb43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2058b03f0644fa9b0ebee648d83dba5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4e038bbec844d8893903b734bde796e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84c75fd8686c4aadb6014d3eb5917a75","IPY_MODEL_7d3ce829858d46128c90e4689ef23f2a","IPY_MODEL_9feffa25bbe64123b8aa8548bbc4da4e"],"layout":"IPY_MODEL_e9f50bb606b2407da8779967e07ebebb"}},"84c75fd8686c4aadb6014d3eb5917a75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e3dc1ea7ee4fae957201519e059e30","placeholder":"​","style":"IPY_MODEL_b4a44981a04346b384820936155f4f14","value":"#4:   0%"}},"7d3ce829858d46128c90e4689ef23f2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_673d988d4ef14400899ac7f7e4160aa1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f31177447954351b730016b370480ad","value":0}},"9feffa25bbe64123b8aa8548bbc4da4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5afd59d39d6b4327b758e6f22d6553ee","placeholder":"​","style":"IPY_MODEL_87ca1969bc6e4e3aab83270fd168f7c8","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"e9f50bb606b2407da8779967e07ebebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73e3dc1ea7ee4fae957201519e059e30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4a44981a04346b384820936155f4f14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"673d988d4ef14400899ac7f7e4160aa1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f31177447954351b730016b370480ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5afd59d39d6b4327b758e6f22d6553ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87ca1969bc6e4e3aab83270fd168f7c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d8c815dd5d24860ae2deb89462ec31d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6710babe057446d0999b5d1beea65c29","IPY_MODEL_247c4a8170ec4e78ad40dee8f7402f11","IPY_MODEL_0d856956ccf347e8bd40053a3bbbc7fd"],"layout":"IPY_MODEL_907ffde14fad4d40973a90999d3218b8"}},"6710babe057446d0999b5d1beea65c29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd439fc0866941148aedbe493ba1a00b","placeholder":"​","style":"IPY_MODEL_11a1c25d7fbd4ba7ac1a8a4b0ee74952","value":"#5:   0%"}},"247c4a8170ec4e78ad40dee8f7402f11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_40484f5ceae1431f9186af93590c1886","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f21ff43af7649838211f42a64d524b8","value":0}},"0d856956ccf347e8bd40053a3bbbc7fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f500b69df2404bf68571bf8589a9689f","placeholder":"​","style":"IPY_MODEL_573d0209bdc04ce89c85738c7019838e","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"907ffde14fad4d40973a90999d3218b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd439fc0866941148aedbe493ba1a00b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11a1c25d7fbd4ba7ac1a8a4b0ee74952":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40484f5ceae1431f9186af93590c1886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f21ff43af7649838211f42a64d524b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f500b69df2404bf68571bf8589a9689f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"573d0209bdc04ce89c85738c7019838e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90941f7fed3747e6afb1f729ba70bf61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f9988fb555f49419a6fe1349a3445c3","IPY_MODEL_ea807c2e37384a2a8d3037a391b8540f","IPY_MODEL_cfd89566b0694812a462b2a4aec248d8"],"layout":"IPY_MODEL_f7ad3757eab44982806d95d7bb171820"}},"1f9988fb555f49419a6fe1349a3445c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_258239f2217942e4b931ecc592489acc","placeholder":"​","style":"IPY_MODEL_567e9be41ae24c238dbf3f1658844769","value":"#6:   0%"}},"ea807c2e37384a2a8d3037a391b8540f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7c2dc0e76994728aed456d041f3b270","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_856991b6825f4608912fd0991b36d1df","value":0}},"cfd89566b0694812a462b2a4aec248d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1941150119df403cb08e09fcd686dcec","placeholder":"​","style":"IPY_MODEL_4fe635b110ae4bc3aa38672ded4a5cd5","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"f7ad3757eab44982806d95d7bb171820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"258239f2217942e4b931ecc592489acc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"567e9be41ae24c238dbf3f1658844769":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7c2dc0e76994728aed456d041f3b270":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"856991b6825f4608912fd0991b36d1df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1941150119df403cb08e09fcd686dcec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fe635b110ae4bc3aa38672ded4a5cd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83a2e7956e72425d87cdccf04cf631f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58dd7d6182264a3d939077d937c4cbf8","IPY_MODEL_6037fb0f8daf4d34860830ccee1b4329","IPY_MODEL_eee007e13a054d67a05a6a4ae6b4823c"],"layout":"IPY_MODEL_f84b3e9eaac14d75800b8bc27d6c48d0"}},"58dd7d6182264a3d939077d937c4cbf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_213bf50f085240f7a22c7e4770cb056d","placeholder":"​","style":"IPY_MODEL_6b29f6536d814f6dab46cb67a44da263","value":"#7:   0%"}},"6037fb0f8daf4d34860830ccee1b4329":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_63ecf3afaebb4b9da54a8c5c310bcf7e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a25b5d535a5d40ecab3d315a7027f170","value":0}},"eee007e13a054d67a05a6a4ae6b4823c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b6528b5ee2f4b2f9a3a6b1212536366","placeholder":"​","style":"IPY_MODEL_b904ab29e9004ad6bad22d8beab9b36a","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"f84b3e9eaac14d75800b8bc27d6c48d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"213bf50f085240f7a22c7e4770cb056d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b29f6536d814f6dab46cb67a44da263":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63ecf3afaebb4b9da54a8c5c310bcf7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a25b5d535a5d40ecab3d315a7027f170":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b6528b5ee2f4b2f9a3a6b1212536366":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b904ab29e9004ad6bad22d8beab9b36a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"332c270dd1f448ea9ccb000a1fc11b8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31df1feace324aceb64bdfa2b9dbab2e","IPY_MODEL_600a11007e944d5c9dd54317100bee7e","IPY_MODEL_5a9c8507a668432c8deedac2290979b0"],"layout":"IPY_MODEL_2a42457d6b6b41cda8159aea9db0e706"}},"31df1feace324aceb64bdfa2b9dbab2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_113822b5f9694d2e89018fefc92ae632","placeholder":"​","style":"IPY_MODEL_f70ac3e6c6714c63b9dfc4266f1f05b9","value":"#0:   0%"}},"600a11007e944d5c9dd54317100bee7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5a02fd843f343ba9c4f530e3577a586","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_630556acc6cc436b9a8fb78b0a76f23a","value":0}},"5a9c8507a668432c8deedac2290979b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05a420139c3944a9a9616d90867a61ea","placeholder":"​","style":"IPY_MODEL_77a3f18c6520445f922f73bb7e8be1ae","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"2a42457d6b6b41cda8159aea9db0e706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"113822b5f9694d2e89018fefc92ae632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f70ac3e6c6714c63b9dfc4266f1f05b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5a02fd843f343ba9c4f530e3577a586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"630556acc6cc436b9a8fb78b0a76f23a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05a420139c3944a9a9616d90867a61ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77a3f18c6520445f922f73bb7e8be1ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54303d7820464dac8319cbf94949efa0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6980009402084edbb10a7ca856672de2","IPY_MODEL_aa38051f32114a4eab9ed1c945ac0672","IPY_MODEL_3b3bb9f85b4d40278431b605a410fb73"],"layout":"IPY_MODEL_2553ae13f47249dbad99adabcefe7111"}},"6980009402084edbb10a7ca856672de2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7689980e9c4c4254865b61fd4a8f2e03","placeholder":"​","style":"IPY_MODEL_e319251b20fb4732afdfcb2f1ae084c0","value":"#1:   0%"}},"aa38051f32114a4eab9ed1c945ac0672":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_8be1d30893b74e5190a0c1c32967d5e1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21d28b1f0be74a4895458cfa5f9b2eea","value":0}},"3b3bb9f85b4d40278431b605a410fb73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af4e052cbe6041fdb10f9d29cf4f8831","placeholder":"​","style":"IPY_MODEL_f395d9913a3347f98bc2e1753272a7cf","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"2553ae13f47249dbad99adabcefe7111":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7689980e9c4c4254865b61fd4a8f2e03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e319251b20fb4732afdfcb2f1ae084c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8be1d30893b74e5190a0c1c32967d5e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21d28b1f0be74a4895458cfa5f9b2eea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af4e052cbe6041fdb10f9d29cf4f8831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f395d9913a3347f98bc2e1753272a7cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a403ad8af45444a596177802194f84d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63b1c789bb514bd9bb3be72ed399fc04","IPY_MODEL_8337430a619a4dfd8cfd1927114bdc53","IPY_MODEL_05d4c5de974e49da8b1e0968c520c56e"],"layout":"IPY_MODEL_116a0db5397c44f99f2b4916a439b99d"}},"63b1c789bb514bd9bb3be72ed399fc04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_030c12f6865b4c329323f3856b0614f9","placeholder":"​","style":"IPY_MODEL_d2a83cd1b20f417b8dfec33944f04d59","value":"#2:   0%"}},"8337430a619a4dfd8cfd1927114bdc53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_097c35e4f7e146779d4b0fc2116b1a48","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a4cf313faf14c5593d24bbe0918f134","value":0}},"05d4c5de974e49da8b1e0968c520c56e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa6553f6ef44c61b3a0a50467f287b5","placeholder":"​","style":"IPY_MODEL_1036a0bf49e94ef59e73db5ca46ace27","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"116a0db5397c44f99f2b4916a439b99d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"030c12f6865b4c329323f3856b0614f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2a83cd1b20f417b8dfec33944f04d59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"097c35e4f7e146779d4b0fc2116b1a48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a4cf313faf14c5593d24bbe0918f134":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fa6553f6ef44c61b3a0a50467f287b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1036a0bf49e94ef59e73db5ca46ace27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f082adfc4f145e59af2f60e52189010":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5031fb3440fb41b6ae79c47c4ccae8e7","IPY_MODEL_8b297d633fd746c596adca246a92c6d6","IPY_MODEL_7e9065525bda4d07925c70041cadbb43"],"layout":"IPY_MODEL_de3f2371eadc451fb5b23cbf5dc54341"}},"5031fb3440fb41b6ae79c47c4ccae8e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95f83995ef154dcf8e5c54630211af09","placeholder":"​","style":"IPY_MODEL_0ad9c0d825ee455fbdd14a6128e661fa","value":"#3:   0%"}},"8b297d633fd746c596adca246a92c6d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aaec25f3f8c4645843ef89f172b246f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0c916a3ff884e74abd2c07da74b3bf3","value":0}},"7e9065525bda4d07925c70041cadbb43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14ff06fcb82f4c898ec7b4828edb8993","placeholder":"​","style":"IPY_MODEL_3df8e595688746a49c529121e5ec14c1","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"de3f2371eadc451fb5b23cbf5dc54341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95f83995ef154dcf8e5c54630211af09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ad9c0d825ee455fbdd14a6128e661fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aaec25f3f8c4645843ef89f172b246f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0c916a3ff884e74abd2c07da74b3bf3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14ff06fcb82f4c898ec7b4828edb8993":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3df8e595688746a49c529121e5ec14c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5470884b2cfd4d77994ffd4a2cb5926c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed2ab8ca836a4d24b950e6d4d8538134","IPY_MODEL_93cf721b4b4e4a13a151bd8c102ae5d7","IPY_MODEL_1cf9235c23f34f38bb401b6f1b5c368c"],"layout":"IPY_MODEL_834469d366ef426c97bbf1f5c282cebc"}},"ed2ab8ca836a4d24b950e6d4d8538134":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbf7597adef74d369750e6cf6fe6a073","placeholder":"​","style":"IPY_MODEL_64834fd572da4b0bafa5ea42e03e02d4","value":"#4:   0%"}},"93cf721b4b4e4a13a151bd8c102ae5d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4ebfc2748ad49d4bbec3dee027e7bfe","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b43a0f0b5d314eb091c0fe5b20772046","value":0}},"1cf9235c23f34f38bb401b6f1b5c368c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_693cce5d69724a488c20494ea6ef0103","placeholder":"​","style":"IPY_MODEL_02f2406ac792413597b1c95113354f70","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"834469d366ef426c97bbf1f5c282cebc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbf7597adef74d369750e6cf6fe6a073":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64834fd572da4b0bafa5ea42e03e02d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4ebfc2748ad49d4bbec3dee027e7bfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b43a0f0b5d314eb091c0fe5b20772046":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"693cce5d69724a488c20494ea6ef0103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02f2406ac792413597b1c95113354f70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02001d2c16a64aa7b858939c760ee238":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07f3bc307c46466d8a5031de0eea951b","IPY_MODEL_115111a9c1d747e3833be819d02e6315","IPY_MODEL_26a87947e701427a837a00cf8e97c3d6"],"layout":"IPY_MODEL_c02bc35b73cd47f5b364e0c865e711fb"}},"07f3bc307c46466d8a5031de0eea951b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a61aec91787241d5a5d673e2be6af3ab","placeholder":"​","style":"IPY_MODEL_b592c48df84a40dd861e55ad3156e625","value":"#5:   0%"}},"115111a9c1d747e3833be819d02e6315":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_589fd522bd7541d8b774e2ef7766f4e9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48db43f14fea49f985f784589045aec3","value":0}},"26a87947e701427a837a00cf8e97c3d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e62a00838bb42c8a63a91fa272a33cd","placeholder":"​","style":"IPY_MODEL_d80c9a8063d64712a51d49f0d06a5dc5","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"c02bc35b73cd47f5b364e0c865e711fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a61aec91787241d5a5d673e2be6af3ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b592c48df84a40dd861e55ad3156e625":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"589fd522bd7541d8b774e2ef7766f4e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48db43f14fea49f985f784589045aec3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e62a00838bb42c8a63a91fa272a33cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d80c9a8063d64712a51d49f0d06a5dc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a5ab24a484e4d8984102fe9933ce47e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6df2aa00f03a4206b9ff1a940eec14a3","IPY_MODEL_d756e905c6de490d9f5b620b215a23be","IPY_MODEL_3b9c456eddee4f258766be902e2bbb9c"],"layout":"IPY_MODEL_5b0ab8780efa4da5979a39275a17e35f"}},"6df2aa00f03a4206b9ff1a940eec14a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_490f511eafd24468a4764fcfa2286f4d","placeholder":"​","style":"IPY_MODEL_f38f2ea972b54778bd1f4a63f2c791de","value":"#6:   0%"}},"d756e905c6de490d9f5b620b215a23be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1f059c9906143ff8342e486d2b25e12","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ff8a1c7fb8744899de90d2d1cf2e2b5","value":0}},"3b9c456eddee4f258766be902e2bbb9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a2b83695bd2478f8e907e367e709a27","placeholder":"​","style":"IPY_MODEL_ad1fa25aa6624b9ebc1d661bb62a95bb","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"5b0ab8780efa4da5979a39275a17e35f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"490f511eafd24468a4764fcfa2286f4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f38f2ea972b54778bd1f4a63f2c791de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1f059c9906143ff8342e486d2b25e12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ff8a1c7fb8744899de90d2d1cf2e2b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a2b83695bd2478f8e907e367e709a27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad1fa25aa6624b9ebc1d661bb62a95bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce293de3d4dd4034959e48ec6edbb9af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fbc321a996a45d493c9f27bca8b6335","IPY_MODEL_3c1a3b8e65ee46ac90e980ea4b87a535","IPY_MODEL_d9d5e76c54a446fab57d195b39acc36b"],"layout":"IPY_MODEL_db2a52a9ecf448feb46f67e77f68c1ae"}},"8fbc321a996a45d493c9f27bca8b6335":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f5f8139f43244b18def8e947bea2595","placeholder":"​","style":"IPY_MODEL_61980037b35842378fa7ffb732306d31","value":"#7:   0%"}},"3c1a3b8e65ee46ac90e980ea4b87a535":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c9ce20ae0894ebdb26d432559c1093a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfee0bfddee841b6b8c0f364a46e103f","value":0}},"d9d5e76c54a446fab57d195b39acc36b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ebf651e257141289349837be9f36622","placeholder":"​","style":"IPY_MODEL_d0434c62e0c64f8aa9dd8ea7fc6016ec","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"db2a52a9ecf448feb46f67e77f68c1ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f5f8139f43244b18def8e947bea2595":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61980037b35842378fa7ffb732306d31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c9ce20ae0894ebdb26d432559c1093a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfee0bfddee841b6b8c0f364a46e103f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ebf651e257141289349837be9f36622":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0434c62e0c64f8aa9dd8ea7fc6016ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb848d61ef80497fbff3e4dd14c1f7bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46f98c3cf0a04204bf7a0e69abd6e46a","IPY_MODEL_5c8fe20fc0f9462191b3f7e34ae36362","IPY_MODEL_74fc4846390a4f61ba0f26874085e116"],"layout":"IPY_MODEL_0fed2ee156b741368a45f81fdc3875ad"}},"46f98c3cf0a04204bf7a0e69abd6e46a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e025dfc7191f4e8dba38afe26b5811e9","placeholder":"​","style":"IPY_MODEL_f1de8a4467a6496c9225fd739c28224f","value":"Downloading: 100%"}},"5c8fe20fc0f9462191b3f7e34ae36362":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d092278dbe104976833ad038400c4266","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e5ba414c6a74fd29a49dfe3889a1249","value":440473133}},"74fc4846390a4f61ba0f26874085e116":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_739a790535d04a1ab2e80326c18a9219","placeholder":"​","style":"IPY_MODEL_b49f4a06adb548578e9f7565be49b46e","value":" 440M/440M [00:14&lt;00:00, 22.6MB/s]"}},"0fed2ee156b741368a45f81fdc3875ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e025dfc7191f4e8dba38afe26b5811e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1de8a4467a6496c9225fd739c28224f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d092278dbe104976833ad038400c4266":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e5ba414c6a74fd29a49dfe3889a1249":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"739a790535d04a1ab2e80326c18a9219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b49f4a06adb548578e9f7565be49b46e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52c81e558f8b4e929f53d8ddad56f3fc":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_689dcd81ff8b410ea453f433568e9d2a","IPY_MODEL_61c3af61d07e415b8ea2b3721a69eca0"],"layout":"IPY_MODEL_b1593a6be59c4594aa06ab5d0a3ab7d8"}},"689dcd81ff8b410ea453f433568e9d2a":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f12d745ef1b4844b8d798ba48c17938","placeholder":"​","style":"IPY_MODEL_b09dd9ff585e4ddf805591ae1643201d","value":"0.446 MB of 0.446 MB uploaded (0.000 MB deduped)\r"}},"61c3af61d07e415b8ea2b3721a69eca0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c914155f4284b2cb3ac671836d47cde","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb72a5e365224f4f9f1120cc85898fb1","value":1}},"b1593a6be59c4594aa06ab5d0a3ab7d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f12d745ef1b4844b8d798ba48c17938":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b09dd9ff585e4ddf805591ae1643201d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c914155f4284b2cb3ac671836d47cde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb72a5e365224f4f9f1120cc85898fb1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4bd9d5f54de34f4fb79c221043da5d3c":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_74837c56b5c641eaa32d0600b57a0bc2","IPY_MODEL_76db22ef57ba493080ab0f40434cb29f"],"layout":"IPY_MODEL_8eab30c108584766bec300fe942b28ab"}},"74837c56b5c641eaa32d0600b57a0bc2":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92a41a50a18b45d9ad9234f7fd705a8f","placeholder":"​","style":"IPY_MODEL_1701eb13a7f8467aa7e2d494e5fe9edd","value":"0.499 MB of 0.499 MB uploaded (0.000 MB deduped)\r"}},"76db22ef57ba493080ab0f40434cb29f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_75b4f122b7d04140a7e9d97f3825c3cf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3d25412508c4b7d9b8818cce7b182ec","value":1}},"8eab30c108584766bec300fe942b28ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92a41a50a18b45d9ad9234f7fd705a8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1701eb13a7f8467aa7e2d494e5fe9edd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75b4f122b7d04140a7e9d97f3825c3cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d25412508c4b7d9b8818cce7b182ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7cc731d448b4315aa4519a68464db45":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_5f6b62382f2944599e80f6c543c84a63","IPY_MODEL_0f287f5730cd4675b05b7bb8f33065f0"],"layout":"IPY_MODEL_ce727309388a4901868b5d36892c0a25"}},"5f6b62382f2944599e80f6c543c84a63":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fa54939b6754947ad306b428485676e","placeholder":"​","style":"IPY_MODEL_468afdc3f2b84ca78f80a69fb8b1cb52","value":"0.553 MB of 0.553 MB uploaded (0.000 MB deduped)\r"}},"0f287f5730cd4675b05b7bb8f33065f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_216700065fdc40d582e19446d3e32de4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b686c08581e94befb934327971fc65e2","value":1}},"ce727309388a4901868b5d36892c0a25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa54939b6754947ad306b428485676e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"468afdc3f2b84ca78f80a69fb8b1cb52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"216700065fdc40d582e19446d3e32de4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b686c08581e94befb934327971fc65e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c85978db29694ff0818e9c738f13c9d5":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_e6eff6e7cc2c46f7aca69ff18df4e9e0","IPY_MODEL_8e602a229c0640e68d91c6f66b864246"],"layout":"IPY_MODEL_08f598503db240c2855981aece2c5cd3"}},"e6eff6e7cc2c46f7aca69ff18df4e9e0":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5b4feb4c3f24dbbadec7e8b0e61e5f6","placeholder":"​","style":"IPY_MODEL_784ab70369fb47d595cab5c4de2cf3de","value":"0.650 MB of 0.650 MB uploaded (0.000 MB deduped)\r"}},"8e602a229c0640e68d91c6f66b864246":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd46cb32a6254d0bafba3824e42d0de0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad69d2a341bd4892941cd49276c04f23","value":1}},"08f598503db240c2855981aece2c5cd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5b4feb4c3f24dbbadec7e8b0e61e5f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"784ab70369fb47d595cab5c4de2cf3de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd46cb32a6254d0bafba3824e42d0de0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad69d2a341bd4892941cd49276c04f23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}