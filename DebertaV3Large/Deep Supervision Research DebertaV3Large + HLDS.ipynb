{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6QrLCOTjzzW"
   },
   "outputs": [],
   "source": [
    "!pip install -q wandb\n",
    "!pip install -q transformers\n",
    "!pip install -q torch\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmus-ntxUXLM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4OW0qKyjzzY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import wandb\n",
    "import random, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import re\n",
    "from torch.nn import Module\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import sys\n",
    "import gc\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# From this Gist: https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3401,
     "status": "ok",
     "timestamp": 1667091139082,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "lYX_09FZjzzc",
    "outputId": "f214870a-dc65-45b6-d931-99565d3311ea"
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"seed\": [83, 55, 48],\n",
    "    \"model_name\": \"microsoft/deberta-v3-large\",\n",
    "    \"max_length\": 512,\n",
    "    \"lr\": 2e-5, \n",
    "    \"output_lr\": 5e-5,\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    # REPLACE WITH WHATEVER GLUE DATASET WANTED\n",
    "    \"dataset\": \"rte\",\n",
    "    \"type\": \"+ DS + CLS + AAM + Last\",\n",
    "\n",
    "    \"patience\": 6,\n",
    "    \"dropout\": 0.5,\n",
    "    \"grad_accum\": 4,\n",
    "    \"layer_start\": 16,\n",
    "    \"pooler\": \"deep\",\n",
    "    \"aux_weight\": 0.5,\n",
    "\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLRAH4l_dpVE"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW2NU8WKaprn"
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"glue\", CFG[\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667085216238,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "rk66iMNU9bLY",
    "outputId": "bd721dee-2d7d-48eb-da89-4de47727ddc4"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1787,
     "status": "ok",
     "timestamp": 1667085218021,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "gxUD8XrX3SuY",
    "outputId": "2e4742e3-7be6-40aa-916d-16dbc51707c6"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG[\"model_name\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3641,
     "status": "ok",
     "timestamp": 1667085221654,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "ESygDI-J96YR",
    "outputId": "70a23a65-052e-4d7c-dd01-90d03297e38d"
   },
   "outputs": [],
   "source": [
    "data = data.map(lambda data: tokenizer(data[\"sentence1\"], data[\"sentence2\"], padding=True, max_length = CFG[\"max_length\"], truncation = True, return_token_type_ids = False), batched = True, remove_columns = [\"sentence1\", \"sentence2\", \"idx\"], num_proc = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmUuQQt53MKE"
   },
   "outputs": [],
   "source": [
    "train = data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxS2Zq-j9Uw9"
   },
   "outputs": [],
   "source": [
    "val = data[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ylZ0EqF9dtG"
   },
   "outputs": [],
   "source": [
    "test = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667085221655,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "WUfekJjTAHlo",
    "outputId": "a6eaa5f4-6f19-4d3e-9572-0e39a2c8f58b"
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667085221655,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "pLB37gJZCr3V",
    "outputId": "f67b1dcc-af3e-48a7-d8d4-000cb4fdb67f"
   },
   "outputs": [],
   "source": [
    "train.with_format(\"torch\", device = device)\n",
    "val.with_format(\"torch\", device = device)\n",
    "test.with_format(\"torch\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667085221655,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "w-Fzlf-7jzzb",
    "outputId": "92c3c316-911e-4cd0-9c87-f6d5a90f4394"
   },
   "outputs": [],
   "source": [
    "os.environ[\"XRT_TPU_CONFIG\"] = \"tpu_worker;0;10.0.0.2:8470\"\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Deep Supervision Research DebertaV3Large + HLDS.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = \"YOUR_API_KEY\"\n",
    "%env \"WANDB_API_KEY\" \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1992,
     "status": "ok",
     "timestamp": 1667085223641,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "AlrcXScDjzzb",
    "outputId": "d604bbbe-2fef-4c57-9020-4e6a99c4124e"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9ao6YGGsHZfn"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BKcj-aeRXLdx"
   },
   "outputs": [],
   "source": [
    "class AccuracyTracker(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.num_correct = 0\n",
    "        self.total = 0\n",
    "        self.accuracy = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.num_correct += val\n",
    "        self.total += n\n",
    "        self.accuracy = self.num_correct / self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EIj0e07d8crC"
   },
   "outputs": [],
   "source": [
    "#OBVIOUSLY, CHANGE THIS AS YOU NEED. USE SELF.LOG FOR ALL IMPORTANT METRICS\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config, vocab_length, data_loader_len):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.vocab_length = vocab_length\n",
    "        self.base_model = AutoModel.from_pretrained(self.config[\"model_name\"], output_hidden_states = True)  \n",
    "        self.base_model.resize_token_embeddings(vocab_length)\n",
    "        self.base_model = self.base_model\n",
    "\n",
    "        self.fc = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "        self.span = self.base_model.config.num_hidden_layers - self.config[\"layer_start\"] + 1\n",
    "\n",
    "\n",
    "        # self.output_weights = nn.Parameter(\n",
    "        #         torch.tensor([1] * (self.base_model.config.num_hidden_layers + 1 - self.config[\"layer_start\"]), dtype=torch.float)\n",
    "        # )\n",
    "\n",
    "        self.fcs = []\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=CFG[\"dropout\"])\n",
    "\n",
    "        if self.config[\"pooler\"] == \"deep\":\n",
    "            for _ in range(self.span):\n",
    "            # for _ in range(self.base_model.config.num_hidden_layers - self.base_model.config.num_hidden_layers // 2 + 1):\n",
    "                layer = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "                self._init_weights(layer)\n",
    "                self.fcs.append(layer)\n",
    "\n",
    "        self._init_weights(self.fc)\n",
    "        # self._init_weights(self.output_weights)\n",
    "        self.data_loader_len = data_loader_len\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, nn.parameter.Parameter):\n",
    "            module.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "        else:\n",
    "            print(f\"Module of type {type(module)} cannot be initialized\")\n",
    "\n",
    "    def feature(self, inputs):\n",
    "\n",
    "        if self.config[\"pooler\"] == \"deep\":\n",
    "            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        \n",
    "            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"hidden_states\"]\n",
    "\n",
    "            x = torch.stack(x)\n",
    "\n",
    "            return x[self.config[\"layer_start\"]:, :, :, :]\n",
    "            \n",
    "        else:\n",
    "            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        \n",
    "            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"last_hidden_state\"]\n",
    "\n",
    "            return inputs[:, 0, :]\n",
    "\n",
    "            \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        features = self.feature(inputs)\n",
    "\n",
    "        if self.config[\"pooler\"] == \"deep\":\n",
    "\n",
    "            outputs = []\n",
    "\n",
    "            layers = []\n",
    "\n",
    "            for layer_num, layer in enumerate(features):\n",
    "                \n",
    "                layers.append(self.dropout(layer[:, 0, :]))\n",
    "\n",
    "                if layer_num == (len(features) - 1):\n",
    "                    pred = self.fc(layers[-1])\n",
    "                    break\n",
    "            \n",
    "                outputs.append(self.fcs[layer_num](layers[-1]))\n",
    "\n",
    "\n",
    "            outputs = torch.stack(outputs)\n",
    "\n",
    "\n",
    "            # final_cls = torch.sum(self.output_weights.unsqueeze(dim = -1).unsqueeze(dim = -1).expand(layers.shape) * layers, dim = 0) / len(layers)\n",
    "\n",
    "            # pred = self.fc(final_cls)\n",
    "            \n",
    "            return outputs, pred\n",
    "\n",
    "        return self.fc(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPPDgVLsjzzd"
   },
   "source": [
    "# Packaging All The Above Functions into a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lREk9P6jzzg"
   },
   "outputs": [],
   "source": [
    "#CHANGE AS NEEDED. MOST OF THE TIME, PYTORCH'S DEFAULT COLLATOR IS ENOUGH.\n",
    "class DataModule():\n",
    "\n",
    "    def __init__(self, config, train, val, test, collate_fn):\n",
    "        self.config = config\n",
    "        self.train, self.val, self.test = train, val, test\n",
    "        self.collate_fn = collate_fn\n",
    "\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(self.train, batch_size=self.config[\"batch_size\"], shuffle = True, collate_fn = self.collate_fn, pin_memory=True, num_workers = 8)\n",
    "        # train_loader = DataLoader(self.train, batch_size=self.config[\"batch_size\"], shuffle = True, collate_fn = self.collate_fn)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(self.val, batch_size = self.config[\"batch_size\"], collate_fn = self.collate_fn, pin_memory=True, num_workers = 8)\n",
    "        # val_loader = DataLoader(self.val, batch_size = self.config[\"batch_size\"], collate_fn = self.collate_fn)\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(self.test, batch_size = self.config[\"batch_size\"], collate_fn = self.collate_fn, pin_memory=True,  num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmHpjEfcwSC2"
   },
   "outputs": [],
   "source": [
    "def configure_optimizers(config, model):\n",
    "    \n",
    "        if config[\"optimizer\"] == \"AdamW\":\n",
    "            optimizer = AdamW(\n",
    "                model.parameters(), \n",
    "                weight_decay = config[\"weight_decay\"], \n",
    "                lr=config[\"lr\"],\n",
    "                correct_bias = True)\n",
    "\n",
    "        else:\n",
    "            optimizer = AdamW(model.parameters(), weight_decay = config[\"weight_decay\"], lr=config[\"lr\"], correct_bias = True)\n",
    "\n",
    "        if config[\"scheduler\"] == \"cosine\":\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = config[\"num_warmup_steps\"], num_training_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"])\n",
    "\n",
    "        elif config['scheduler'] == \"one_cycle\":\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = config[\"lr\"], pct_start = config[\"pct_start\"], total_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"])\n",
    "            \n",
    "        elif config['scheduler'] == \"linear\":\n",
    "            scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"] * 0.1, num_training_steps = model.data_loader_len * config[\"epochs\"] // config[\"grad_accum\"])\n",
    "            \n",
    "        else:\n",
    "            scheduler = None\n",
    "            \n",
    "        \n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVPtW6szW6JV"
   },
   "outputs": [],
   "source": [
    "def train_fn(config, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "\n",
    "    losses = [AverageMeter() for i in range(model.span + 1)]\n",
    "    overall_loss = AverageMeter()\n",
    "    train_accuracy = AccuracyTracker()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc = f\"Training Loop Epoch: {epoch}\")\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    latest_avg = None\n",
    "        \n",
    "    grad_norm = 0.0\n",
    "\n",
    "    latest_acc = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "\n",
    "        labels = batch.pop(\"labels\")\n",
    "\n",
    "        inputs = batch\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "\n",
    "        labels = labels.to(device).to(torch.float16)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        #First Train\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs, y_hat = model(inputs)\n",
    "\n",
    "            train_loss = criterion(y_hat.flatten(), labels)\n",
    "\n",
    "            losses[-1].update(train_loss, batch_size)\n",
    "\n",
    "            for idx in range(len(outputs) - 1, -1, -1):\n",
    "                aux_loss = config[\"aux_weight\"] * criterion(outputs[idx].flatten(), labels)\n",
    "            \n",
    "                losses[(idx)].update(aux_loss, batch_size)\n",
    "            \n",
    "                train_loss += aux_loss\n",
    "\n",
    "            overall_loss.update(train_loss, batch_size)\n",
    "\n",
    "            scaled_loss = train_loss / config[\"grad_accum\"]\n",
    "        \n",
    "\n",
    "        probs = torch.sigmoid(y_hat)\n",
    "\n",
    "        num_correct = torch.sum((probs.flatten() > 0.5).to(int) == labels)\n",
    "\n",
    "        train_accuracy.update(num_correct, batch_size)\n",
    "        \n",
    "        scaler.scale(scaled_loss).backward()\n",
    "        \n",
    "\n",
    "        if ((batch_idx + 1) % config[\"grad_accum\"] == 0) or (batch_idx + 1 == model.data_loader_len):\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_norm\"])\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if not scheduler is None:\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "            latest_avg = f\"{overall_loss.avg:.4f}\"\n",
    "\n",
    "            latest_acc = f\"{train_accuracy.accuracy:.4f}\"\n",
    "\n",
    "\n",
    "        text = f\"Epoch: {epoch} | Training_accuracy: {latest_acc} | Training Loss_avg: {latest_avg} | Training Loss_step: {overall_loss.val:.4f} | Learning Rate: {scheduler.get_last_lr()[0]:.4f} | Grad: {grad_norm:.4f}\" if not scheduler is None else f\"Epoch: {epoch} | Training Loss_avg: {latest_avg} | Training Loss_step: {overall_loss.val:.4f} | Grad: {grad_norm:.4f}\"\n",
    "\n",
    "        pbar.set_postfix_str(text)\n",
    "\n",
    "        pbar.refresh()\n",
    "\n",
    "        \n",
    "        for idx, loss in enumerate(losses):\n",
    "            if idx < len(losses) - 1:\n",
    "                wandb.log({f\"Training Loss Step Layer {idx + config['layer_start']}\": loss.val})\n",
    "                continue\n",
    "            wandb.log({f\"Training Loss Step Output Layer\": loss.val})\n",
    "\n",
    "        wandb.log({f\"Training Accuracy Step\": train_accuracy.accuracy})\n",
    "\n",
    "    wandb.log({f\"Training Accuracy Epoch\": train_accuracy.accuracy})\n",
    "\n",
    "    wandb.log({f\"Overall Training Loss Epoch\": overall_loss.avg})\n",
    "\n",
    "    return overall_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJhNb7m7XEBn"
   },
   "outputs": [],
   "source": [
    "def valid_fn(config, valid_loader, model, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = [AverageMeter() for i in range(model.span + 1)]\n",
    "    overall_loss = AverageMeter()\n",
    "    accuracy = AccuracyTracker()\n",
    "\n",
    "    #A MANUAL LOOP IS NEEDED HERE SINCE TRAINER FUNCTIONS DON'T GIVE YOU ACCESS TO MODEL PREDICTIONS\n",
    "\n",
    "    pbar = tqdm(valid_loader, desc = f\"Validation Loop Epoch: {epoch}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        \n",
    "        labels = batch.pop(\"labels\")\n",
    "\n",
    "        inputs = batch\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "\n",
    "        labels = labels.to(device).to(torch.float16)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs, y_hat = model(inputs)\n",
    "\n",
    "            val_loss = criterion(y_hat.flatten(), labels)\n",
    "\n",
    "            losses[-1].update(val_loss, batch_size)\n",
    "\n",
    "            for idx in range(len(outputs) - 1, -1, -1):\n",
    "                aux_loss = config[\"aux_weight\"] * criterion(outputs[idx].flatten(), labels)\n",
    "                losses[idx].update(aux_loss, batch_size)\n",
    "                val_loss += aux_loss\n",
    "\n",
    "        overall_loss.update(val_loss.item(), batch_size)\n",
    "\n",
    "        probs = torch.sigmoid(y_hat)\n",
    "\n",
    "        num_correct = torch.sum((probs.flatten() > 0.5).to(int) == labels)\n",
    "        accuracy.update(num_correct, batch_size)\n",
    "\n",
    "        # pbar.set_postfix_str(f\"Epoch: {epoch} | Validation Loss_avg: {losses.avg:.4f} | Validation Loss_step: {losses.val:.4f}\")\n",
    "        pbar.set_postfix_str(f\"Epoch: {epoch} | Validation Loss_avg: {overall_loss.avg:.4f} | Validation_accuracy_step: {accuracy.accuracy}\")\n",
    "    \n",
    "    for idx, loss in enumerate(losses):\n",
    "        if idx < len(losses) - 1:\n",
    "            wandb.log({f\"Validation Loss Epoch Layer {idx + config['layer_start']}\": loss.avg})\n",
    "            continue\n",
    "        wandb.log({f\"Validation Loss Epoch Output Layer\": loss.avg})\n",
    "\n",
    "    wandb.log({\"Overall Validation Loss Epoch\": overall_loss.avg})\n",
    "\n",
    "    wandb.log({f\"Validation Accuracy Epoch\": accuracy.accuracy})\n",
    "\n",
    "    return overall_loss.avg, accuracy.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfC3gU9jxwA9"
   },
   "outputs": [],
   "source": [
    "def test_fn(config, test_loader, model, criterion, device, checkpoint, class_names):\n",
    "    with torch.no_grad():\n",
    "        losses = AverageMeter()\n",
    "        accuracy = AccuracyTracker()\n",
    "\n",
    "        # PUT YOUR CUSTOM GRAPHS HERE\n",
    "        \n",
    "        # LOAD BEST MODEL CHECKPOINT\n",
    "\n",
    "        saved = torch.load(checkpoint)\n",
    "        model.load_state_dict(saved[\"model_state_dict\"])\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        #A MANUAL LOOP IS NEEDED HERE SINCE TRAINER FUNCTIONS DON'T GIVE YOU ACCESS TO MODEL PREDICTIONS\n",
    "\n",
    "        pbar = tqdm(test_loader, desc = f\"Getting Test Predictions\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            labels = batch.pop(\"labels\")\n",
    "\n",
    "            inputs = batch\n",
    "\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "\n",
    "            labels = labels.to(device).to(torch.float16)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "\n",
    "            model = model.to(device)\n",
    "            \n",
    "            outputs, y_hat = model(inputs)\n",
    "\n",
    "            val_loss = criterion(y_hat.flatten(), labels)\n",
    "\n",
    "            for idx, output in enumerate(outputs[:-1]):\n",
    "                val_loss += config[\"aux_weight\"] + criterion(output.flatten(), labels)\n",
    "\n",
    "            probs = torch.sigmoid(y_hat)\n",
    "\n",
    "            num_correct = torch.sum((probs.flatten() > 0.5).to(int) == labels)\n",
    "\n",
    "            losses.update(val_loss, batch_size)\n",
    "\n",
    "            accuracy.update(num_correct, batch_size)\n",
    "\n",
    "            #LOGGING SPECIFIC THINGS HERE #############################\n",
    "\n",
    "    \n",
    "        #LOGGING OOF PERFORMANCE\n",
    "        wandb.log({f\"Validation Accuracy\": accuracy.accuracy})\n",
    "\n",
    "        print(f\"Validation Accuracy: {accuracy.accuracy}\")\n",
    "\n",
    "        return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dqlGGJdqUS0"
   },
   "outputs": [],
   "source": [
    "class ModelTracker():\n",
    "    def __init__(self, patience, base_path, model, path, optimizer, scheduler, mode = \"maximize\", metric_name = \"accuracy\"):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.missed = 0\n",
    "        self.path = path\n",
    "        self.model = model\n",
    "        self.base_path = base_path\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.metric = float(\"-inf\") if self.mode == \"maximize\" else float(\"inf\")\n",
    "        self.metric_name = metric_name\n",
    "\n",
    "    def update(self, value, epoch):\n",
    "        if self.mode == \"maximize\":\n",
    "            if value > self.metric:\n",
    "                print(f\"Validation {self.metric_name} rose from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n",
    "                self.metric = value\n",
    "                \n",
    "                torch.save({\n",
    "                    \"epoch\": epoch, \n",
    "                    \"model_state_dict\": self.model.state_dict(), \n",
    "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                    \"accuracy\": self.metric,\n",
    "                    \"scheduler\": self.scheduler.state_dict()\n",
    "                }, f\"{self.base_path}/{self.path}\")\n",
    "\n",
    "                print(f\"Saved to model to {self.base_path}/{self.path}!\")\n",
    "\n",
    "                self.missed = 0\n",
    "\n",
    "            else:\n",
    "                print(f\"Validation {self.metric_name} fell from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n",
    "                print(f\"Model did not improve on epoch {epoch}\")\n",
    "                self.missed += 1\n",
    "        else:\n",
    "            if value < self.metric:\n",
    "                print(f\"Validation {self.metric_name} fell from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n",
    "                self.metric = value\n",
    "                \n",
    "                torch.save({\n",
    "                    \"epoch\": epoch, \n",
    "                    \"model_state_dict\": self.model.state_dict(), \n",
    "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                    \"loss\": self.metric,\n",
    "                    \"scheduler\": self.scheduler.state_dict()\n",
    "                }, f\"{self.base_path}/{self.path}\")\n",
    "\n",
    "                self.missed = 0\n",
    "\n",
    "                print(f\"Saved to model to {self.base_path}/{self.path}!\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Validation {self.metric_name} rose from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n",
    "                print(f\"Model did not improve on epoch {epoch}\")\n",
    "                self.missed += 1\n",
    "\n",
    "    def get_full_path(self):\n",
    "        return f\"{self.base_path}/{self.path}\"\n",
    "        \n",
    "    def check_improvement(self):\n",
    "        return (self.missed < self.patience if self.mode == \"maximize\" else self.missed > self.patience) and (self.missed < self.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruzVJ42ujzzj"
   },
   "outputs": [],
   "source": [
    "def train_loop(train, val, test, data_collator, config, device, weights=None, base_path = \"./\"):\n",
    "    for seed in config[\"seed\"]:\n",
    "        seed_everything(seed)\n",
    "        classes = [\"negative\", \"positive\"]\n",
    "\n",
    "        config['batch_size'] = config['batch_size'] * config['grad_accum']\n",
    "        wandb.init(project=\"YOUR PROJECT NAME\", entity = \"YOUR USERNAME\", group = config[\"dataset\"], config = config, job_type = f\"{config['model_name']} {config['type']}\", save_code = True, reinit = True, name = f\"Seed {seed}\")\n",
    "\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        validation_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        dataset = DataModule(config, train, val, test, data_collator)\n",
    "\n",
    "        train_loader = dataset.train_dataloader()\n",
    "\n",
    "        val_loader = dataset.val_dataloader()\n",
    "\n",
    "        model = Model(config, len(config[\"tokenizer\"]), len(train_loader))\n",
    "        \n",
    "        model = model.to(device)\n",
    "\n",
    "        for i, fc in enumerate(model.fcs):\n",
    "            model.fcs[i] = fc.to(device)\n",
    "\n",
    "        optimizer, scheduler = configure_optimizers(config, model)\n",
    "\n",
    "        tracker = ModelTracker(config[\"patience\"], base_path, model, f\"seed-{seed}.pt\", optimizer, scheduler)\n",
    "\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "            train_loss = train_fn(config, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "            val_loss, val_accuracy = valid_fn(config, val_loader, model, validation_criterion, device, epoch)\n",
    "\n",
    "            tracker.update(val_accuracy, epoch)\n",
    "\n",
    "            if not tracker.check_improvement():\n",
    "                print(f\"Stopping the model at epoch {epoch} since the model did not improve!\")\n",
    "                break\n",
    "\n",
    "\n",
    "        checkpoint = tracker.get_full_path()\n",
    "\n",
    "        test_fn(config, val_loader, model, validation_criterion, device, checkpoint, classes)\n",
    "\n",
    "        del dataset, model\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGhABbNI2Yz_"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZirworkgOGgS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4057357,
     "status": "error",
     "timestamp": 1667089280987,
     "user": {
      "displayName": "Leo Daniel",
      "userId": "14081355745367355904"
     },
     "user_tz": 240
    },
    "id": "i9jBz0mZUXLZ",
    "outputId": "0e9a753f-a127-4381-d3f1-28b5a07cca01"
   },
   "outputs": [],
   "source": [
    "train_loop(train, val, test, data_collator, CFG, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRuuJZIAYz5U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrfPAilwoOH5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcQCJvBgDGMb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "548_d2-Xyd_-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1ivHMO7WAD738_2NhpmUujhPV5wtG5Uuv",
     "timestamp": 1665970089435
    },
    {
     "file_id": "1iZlCeL0v9o6fEt1wstRlgV371pOB2kgi",
     "timestamp": 1664493650741
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
